{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### classification\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ### graphical plotly basics\n",
    "# import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "# for jupyter notebook display management\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diab_raw = dfc.load_dataset_from_config('diabete_data', sep=',')\n",
    "\n",
    "if df_diab_raw is not None and isinstance(df_diab_raw, pd.DataFrame):\n",
    "    dfc.log_general_info(df_diab_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_diab_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_diab_raw))\n",
    "    df_diab = dfc.normalize_column_names(df_diab_raw)\n",
    "    display(df_diab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diab_desc = df_diab.select_dtypes(include=np.number).describe()\n",
    "display(df_diab_desc)\n",
    "df_diab_cr = df_diab.select_dtypes(include=np.number).corr()\n",
    "display(df_diab_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and dupplicates management\n",
    "df_diab_orig = df_diab.copy()\n",
    "df_diab = df_diab.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 2. Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2.1 General Analysis variable/target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation des variables explicatives (features) et de la variable à prédire (target)\n",
    "data = df_diab.drop(['outcome'], axis=1)\n",
    "target = df_diab['outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation de données d'entrainement et données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=4)\n",
    "print(\"Train Set:\", X_train.shape)\n",
    "print(\"Test Set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modèles unitaires utilisés par les meta modèles\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "clf_RFC = RandomForestClassifier(random_state= 123)\n",
    "clf_LR = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 2.2 Voting Classifier\n",
    "- Aggregate Heterogeneous classifier (models)\n",
    "- Hard (frequence) or Soft (mean) voting with weight in the end to select the good one\n",
    "- /!\\ High CPU consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### 2.2.1 Sans hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clf_Voting = VotingClassifier(estimators=[('KNN',clf_KNN), ('RFC',clf_RFC), ('LR',clf_LR)], voting='hard')\n",
    "clf_Voting.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation croisée sur les données d'entrainement\n",
    "cv3 = KFold(n_splits=3, shuffle = True, random_state=111)\n",
    "# d)\n",
    "for clf, label in zip([clf_KNN, clf_RFC, clf_LR, clf_Voting], ['KNN', 'Random Forest', 'Logistic Regression', 'Voting Classifier']):\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=cv3, scoring=['accuracy','f1'])\n",
    "    print(f\"[{label}]: \\n Accuracy: \"\n",
    "          f\"{scores['test_accuracy'].mean().round(2)} \"\n",
    "          f\"(+/- {scores['test_accuracy'].std().round(2)}) \"\n",
    "          f\"F1 score: {scores['test_f1'].mean().round(2)} \"\n",
    "          f\"(+/- {scores['test_f1'].std().round(2)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clf_Voting.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du modèle sur les données de test\n",
    "y_pred = clf_Voting.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test, pd.Series(y_pred, index=y_test.index), rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clf_Voting.score(X_test, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 2.2.2 Avec hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clf_KNN = KNeighborsClassifier(n_neighbors=3)\n",
    "clf_RFC = RandomForestClassifier(random_state= 123)\n",
    "clf_LR = LogisticRegression(max_iter=1000)\n",
    "clf_Voting = VotingClassifier(estimators=[('KNN',clf_KNN), ('RFC',clf_RFC), ('LR',clf_LR)], voting='hard')\n",
    "clf_Voting.fit(X_train, y_train)\n",
    "\n",
    "parameters = { # syntaxe : \"__\" signifie que les paramètres s'applique au modele nommé dans estimators\n",
    "    'knn__n_neighbors': [5, 9], # Ex : s'applique à knn uniquement\n",
    "    'rfc__n_estimators': [20, 100, 200],\n",
    "    'lr__C': [0.01, 0.1, 1],\n",
    "    'estimators': [[('knn',clf_KNN), ('rfc',clf_RFC), ('lr',clf_LR)]] \n",
    "}\n",
    "\n",
    "grid_clf_Voting = GridSearchCV(estimator=clf_Voting, param_grid=parameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données d'entrainement à l'aide d'une grille de combinaison d'hyperparamètres\n",
    "result_grid = grid_clf_Voting.fit(X_train, y_train)\n",
    "# on peut récupérer le best estimator après l'entrainement ainsi qu'afficher ses paramètres\n",
    "best_clf_Voting = result_grid.best_estimator_\n",
    "print(\"Meilleure combinaison de paramètres trouvée pour les données d'entrainement:\",best_clf_Voting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'efficacité de la grille avec Plotly express\n",
    "df_result_grid = pd.DataFrame.from_dict(result_grid.cv_results_)\n",
    "display(df_result_grid.head())\n",
    "fig = px.bar(\n",
    "    df_result_grid,\n",
    "    x=\"rank_test_score\",      \n",
    "    y=\"mean_test_score\",      \n",
    "    color=\"param_rfc__n_estimators\", \n",
    "    color_continuous_scale=\"plasma_r\",\n",
    "    text=\"rank_test_score\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    title=\"Distribution des scores par rang avec coloration selon le kernel\",\n",
    "    xaxis_title=\"Rang\",\n",
    "    yaxis_title=\"Mean Test Score\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    coloraxis_colorbar=dict(\n",
    "        title=\"n_estimators\",    \n",
    "        tickvals=[20, 100, 200],  \n",
    "        ticktext=[\"20\", \"100\", \"200\"], \n",
    "    )\n",
    ")\n",
    "fig.update_traces(\n",
    "    customdata=df_result_grid[['param_knn__n_neighbors', 'param_rfc__n_estimators', 'param_lr__C']],\n",
    "    hovertemplate=\n",
    "        \"Score moyen: %{y:.4f}<br>\"\n",
    "        \"KNN/Nb Neighbors: %{customdata[0]}<br>\"\n",
    "        \"RFC/Nb Estimators: %{customdata[1]}<br>\"\n",
    "        \"LR/C: %{customdata[2]}<br>\"\n",
    "        \"<extra></extra>\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 2.3 Stacking\n",
    "- meta model based on the predictions of L models on N values, the input for the Stacking model is the predictions only (unless enriched specifically)\n",
    "- /!\\ High CPU consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clf_Stacking = StackingClassifier(estimators=[('KNN',clf_KNN), ('RFC',clf_RFC), ('LR',clf_LR)], final_estimator=clf_LR, cv=5)\n",
    "clf_Stacking.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation croisée sur les données d'entrainement\n",
    "cv3 = KFold(n_splits=3, shuffle = True, random_state=111)\n",
    "# d)\n",
    "for clf, label in zip([clf_KNN, clf_RFC, clf_LR, clf_Stacking], ['KNN', 'Random Forest', 'Logistic Regression', 'Stacking Classifier']):\n",
    "    scores = cross_validate(clf, X_train, y_train, cv=cv3, scoring=['accuracy','f1'])\n",
    "    print(f\"[{label}]: \\n Accuracy: \"\n",
    "          f\"{scores['test_accuracy'].mean().round(2)} \"\n",
    "          f\"(+/- {scores['test_accuracy'].std().round(2)}) \"\n",
    "          f\"F1 score: {scores['test_f1'].mean().round(2)} \"\n",
    "          f\"(+/- {scores['test_f1'].std().round(2)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clf_Stacking.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du modèle sur les données de test\n",
    "y_pred = clf_Stacking.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test.to_numpy().ravel(), y_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clf_Stacking.score(X_test, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
