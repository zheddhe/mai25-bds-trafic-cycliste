{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import cast\n",
    "\n",
    "### classification\n",
    "from sklearn import neighbors\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "### graphical plotly basics\n",
    "import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# for jupyter notebook display management\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits =  cast(Bunch, datasets.load_digits())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# 2. Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 2.1 General Analysis variable/target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation des variables explicatives (features) et de la variable à prédire (target)\n",
    "X_digits = pd.DataFrame(digits.data)\n",
    "display(X_digits.head())\n",
    "y_digits = digits.target\n",
    "display(np.unique(y_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation de données d'entrainement et données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits, train_size=0.8, random_state=126)\n",
    "print(\"Train Set:\", X_train.shape)\n",
    "print(\"Test Set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2.2 KNN Classification (minkowski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clfKNN = neighbors.KNeighborsClassifier(n_neighbors=7, metric='minkowski')\n",
    "clfKNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clfKNN.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du modèle sur les données de test\n",
    "y_test_pred = clfKNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test, y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clfKNN.score(X_test, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 2.2 KNN Classification (Manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clfKNN_man = neighbors.KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
    "clfKNN_man.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clfKNN_man.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du modèle sur les données de test\n",
    "y_test_pred = clfKNN_man.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test, y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clfKNN_man.score(X_test, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'minkowski' : [],\n",
    "    'manhattan' : [],\n",
    "    'chebyshev' : []\n",
    "}\n",
    "for i in range(1,41):\n",
    "    clfKNN_minko = neighbors.KNeighborsClassifier(n_neighbors=i, metric='minkowski')\n",
    "    clfKNN_minko.fit(X_train, y_train)\n",
    "    y_test_pred = clfKNN_minko.predict(X_test)\n",
    "    scores['minkowski'].append(clfKNN_minko.score(X_test, y_test))\n",
    "    clfKNN_man = neighbors.KNeighborsClassifier(n_neighbors=i, metric='manhattan')\n",
    "    clfKNN_man.fit(X_train, y_train)\n",
    "    y_test_pred = clfKNN_man.predict(X_test)\n",
    "    scores['manhattan'].append(clfKNN_man.score(X_test, y_test))\n",
    "    clfKNN_cheb = neighbors.KNeighborsClassifier(n_neighbors=i, metric='chebyshev')\n",
    "    clfKNN_cheb.fit(X_train, y_train)\n",
    "    y_test_pred = clfKNN_cheb.predict(X_test)\n",
    "    scores['chebyshev'].append(clfKNN_cheb.score(X_test, y_test))\n",
    "    print(f\"k={i}:\",scores['minkowski'][i-1], scores['manhattan'][i-1], scores['chebyshev'][i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracer la courbe score en fonction de K pour différents modèles avec Plotly\n",
    "fig = go.Figure()\n",
    "for metric in scores:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=np.arange(1,41,1),\n",
    "        y=scores[metric],\n",
    "        mode='lines',\n",
    "        name=f'Score {metric}',\n",
    "        line=dict(dash='dash', width=3)\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    title=\"Courbes Score en fonction de K pour les modèles KNN\",\n",
    "    xaxis_title=\"valeur de K\",\n",
    "    yaxis_title=\"Score\",\n",
    "    legend_title=\"Métrique\",\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
