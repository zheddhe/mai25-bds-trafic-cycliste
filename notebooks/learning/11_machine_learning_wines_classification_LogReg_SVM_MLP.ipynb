{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### classification\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "### graphical plotly basics\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "# for jupyter notebook display management\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wines_raw = dfc.load_dataset_from_config('wines_data', sep=',')\n",
    "\n",
    "if df_wines_raw is not None and isinstance(df_wines_raw, pd.DataFrame):\n",
    "    dfc.log_general_info(df_wines_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_wines_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_wines_raw))\n",
    "    df_wines = dfc.normalize_column_names(df_wines_raw)\n",
    "    display(df_wines.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wines_desc = df_wines.select_dtypes(include=np.number).describe()\n",
    "display(df_wines_desc)\n",
    "df_wines_cr = df_wines.select_dtypes(include=np.number).corr()\n",
    "display(df_wines_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and dupplicates management\n",
    "df_wines_orig = df_wines.copy()\n",
    "df_wines = df_wines.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# 3. Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 3.1 General Analysis variable/target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Categorisation for first level human eye classification estimation using quartiles\n",
    "malic_acid = pd.cut(\n",
    "    x=df_wines.malic_acid, \n",
    "    bins=[df_wines_desc.malic_acid['min'],\n",
    "          df_wines_desc.malic_acid['25%'],\n",
    "          df_wines_desc.malic_acid['50%'],\n",
    "          df_wines_desc.malic_acid['75%'],\n",
    "          df_wines_desc.malic_acid['max']],\n",
    "    labels=['low', 'medium-', 'medium+', 'high']\n",
    ")\n",
    "display(pd.crosstab(df_wines['class'], malic_acid, normalize='columns'))\n",
    "flavanoids = pd.cut(    \n",
    "    x=df_wines.flavanoids, \n",
    "    bins=[df_wines_desc.flavanoids['min'],\n",
    "          df_wines_desc.flavanoids['25%'],\n",
    "          df_wines_desc.flavanoids['50%'],\n",
    "          df_wines_desc.flavanoids['75%'],\n",
    "          df_wines_desc.flavanoids['max']],\n",
    "    labels=['low', 'medium-', 'medium+', 'high']\n",
    ")\n",
    "display(pd.crosstab(df_wines['class'], flavanoids, normalize='columns'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation des variables explicatives (features) et de la variable à prédire (target)\n",
    "data = df_wines.drop('class', axis=1)\n",
    "target = df_wines['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation de données d'entrainement et données de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, train_size=0.8, random_state=66)\n",
    "print(\"Train Set:\", X_train.shape)\n",
    "print(\"Test Set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression and One Hot Encoder preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing des variables explicatives d'entrainement et de test (encodage de discrétisation pour le machine learning)\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# NB : fit réinitialise l'encodeur avec les catégories et les statistiques des données d'entrainement\n",
    "# inutile de le refaire pour la partie donnée de test donc\n",
    "enc.fit(X_train)\n",
    "X_train_enc = enc.transform(X_train)\n",
    "# X_train_enc = encoder.fit_transform(X_train)\n",
    "X_test_enc = enc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle\n",
    "clfLR = linear_model.LogisticRegression(C=1.0)\n",
    "clfLR.fit(X_train_enc, y_train.to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clfLR.score(X_train_enc, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction du modèle sur les données de test\n",
    "y_test_pred = clfLR.predict(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test.to_numpy().ravel(), y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clfLR.score(X_test_enc, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optionel] Récupération des probabilités (et remaniement des règles d'identification de classe : par défaut la classe prédite est celle avec la proba la plus elevée)\n",
    "y_probs = clfLR.predict_proba(X_test_enc)\n",
    "# print(y_probs)\n",
    "y_pred_prob_class1 = np.where(\n",
    "    (y_probs[:, 0] >= y_probs[:, 1]) & (y_probs[:, 0] >= y_probs[:, 2]),1,0)\n",
    "y_pred_prob_class2 = np.where(\n",
    "    (y_probs[:, 1] >= y_probs[:, 0]) & (y_probs[:, 1] >= y_probs[:, 2]),1,0)\n",
    "y_pred_prob_class3 = np.where(\n",
    "    (y_probs[:, 2] > y_probs[:, 0]) & (y_probs[:, 2] >= y_probs[:, 1]),1,0)\n",
    "print(\"Application manuelle des règles\\n\",\n",
    "      y_pred_prob_class1,\"\\n\",\n",
    "      y_pred_prob_class2,\"\\n\",\n",
    "      y_pred_prob_class3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de l'AUC et visualisation avec la courbe ROC (pour Receiver Operating Characteristic)\n",
    "\n",
    "# y_pred_bin : vraies classes, par exemple [1, 2, 3, 1, 3]\n",
    "# y_probs : probabilités prédites, de forme (n_samples, n_classes)\n",
    "n_classes = len(df_wines['class'].unique())\n",
    "classes=[i for i in range(1,n_classes+1)]\n",
    "y_pred_bin = label_binarize(y_test, classes=classes)  # one-hot\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "roc_auc = {}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.asarray(y_pred_bin)[:, i], y_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# # Optionnel : macro-average\n",
    "# all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "# mean_tpr = np.zeros_like(all_fpr)\n",
    "# for i in range(n_classes):\n",
    "#     mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "# mean_tpr /= n_classes\n",
    "# macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "# Tracer la courbe ROC+AUC avec Plotly\n",
    "fig = go.Figure()\n",
    "# Ajout de la diagonale (chance)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[0, 1],\n",
    "    y=[0, 1],\n",
    "    mode='lines',\n",
    "    name='Aléatoire',\n",
    "    line=dict(dash='dash')\n",
    "))\n",
    "# Ajout des courbes ROC et AUC\n",
    "for i in range(n_classes):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=fpr[i],\n",
    "        y=tpr[i],\n",
    "        mode='lines',\n",
    "        name=f\"Classe {i} (AUC = {roc_auc[i]:.2f})\"\n",
    "    ))\n",
    "# Mise en forme\n",
    "fig.update_layout(\n",
    "    title=\"Courbes ROC multi-classes (One-vs-Rest)\",\n",
    "    xaxis_title=\"Taux de faux positifs\",\n",
    "    yaxis_title=\"Taux de vrais positifs\",\n",
    "    legend_title=\"Classes\",\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 3.3 Support Vector Machine (SVM) with scaler preprocessing and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing des variables explicatives d'entrainement et de test (scaler)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3.3.1 Sans hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle \n",
    "clfSVM = svm.SVC(gamma=0.01,  kernel='poly')\n",
    "clfSVM.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", clfSVM.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données de test\n",
    "y_test_pred = clfSVM.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test.to_numpy().ravel(), y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", clfSVM.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### 3.3.2 Avec hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyper paramètres\n",
    "parameters = {\n",
    "    'C':[0.1,1,10],\n",
    "    'kernel': ['rbf', 'linear','poly'],\n",
    "    'gamma':[0.001, 0.1, 0.5]\n",
    "}\n",
    "grid_clfSVM = GridSearchCV(estimator=clfSVM, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données d'entrainement à l'aide d'une grille de combinaison d'hyperparamètres\n",
    "result_grid = grid_clfSVM.fit(X_train_scaled, y_train)\n",
    "# on peut récupérer le best estimator après l'entrainement ainsi qu'afficher ses paramètres\n",
    "best_clfSVM = result_grid.best_estimator_\n",
    "print(\"Meilleure combinaison de paramètres trouvée pour les données d'entrainement:\",result_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'efficacité de la grille avec Plotly express\n",
    "df_result_grid = pd.DataFrame.from_dict(result_grid.cv_results_)\n",
    "df_result_grid[\"params_str\"] = df_result_grid[\"params\"].apply(lambda d: str(d))\n",
    "df_result_grid[\"mts_str\"] = df_result_grid[\"mean_test_score\"].round(3)\n",
    "fig3 = px.bar(\n",
    "    df_result_grid,\n",
    "    x=\"rank_test_score\",      \n",
    "    y=\"mean_test_score\",      \n",
    "    color=\"param_kernel\", \n",
    "    text=\"rank_test_score\",\n",
    "    hover_data={\n",
    "        \"params_str\": True,           \n",
    "        \"mean_test_score\": True,      \n",
    "        \"rank_test_score\": False,     \n",
    "        \"param_kernel\": False,          \n",
    "        \"mts_str\": False             \n",
    "    })\n",
    "fig3.update_layout(\n",
    "    title=\"Distribution des scores par rang avec coloration selon le kernel\",\n",
    "    xaxis_title=\"Rang\",\n",
    "    yaxis_title=\"Mean Test Score\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    legend_title='Kernel utilisé',\n",
    ")\n",
    "\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", grid_clfSVM.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données de test\n",
    "y_test_pred = grid_clfSVM.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test.to_numpy().ravel(), y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", grid_clfSVM.score(X_test_scaled, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 3.4 Multi-layer Perceptrons (MLP) with scaler preprocessing and Bayes search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing des variables explicatives d'entrainement et de test (scaler)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition et Entrainement du modèle \n",
    "clf_mlp = MLPClassifier(early_stopping=True)\n",
    "clf_mlp.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 3.4.1 Avec hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des hyper paramètres\n",
    "parameters = {\n",
    "    'hidden_layer_sizes':(50, 500),\n",
    "    'max_iter': (1000, 5000),\n",
    "    'alpha': (0.001, 1),\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "}\n",
    "meta_bscv = BayesSearchCV(estimator=clf_mlp, search_spaces=parameters, n_iter=20, cv=5, scoring='accuracy', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données d'entrainement à l'aide d'une grille de combinaison d'hyperparamètres\n",
    "result_grid = meta_bscv.fit(X_train_scaled, y_train)\n",
    "# on peut récupérer le best estimator après l'entrainement ainsi qu'afficher ses paramètres\n",
    "best_clf_mlp = result_grid.best_estimator_ # type: ignore\n",
    "print(\"Meilleure combinaison de paramètres trouvée pour les données d'entrainement:\",result_grid.best_estimator_) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'efficacité de la grille avec Plotly express\n",
    "df_result_grid = pd.DataFrame.from_dict(result_grid.cv_results_)\n",
    "df_result_grid[\"params_str\"] = df_result_grid[\"params\"].apply(lambda d: str(d))\n",
    "df_result_grid[\"mts_str\"] = df_result_grid[\"mean_test_score\"].round(3)\n",
    "fig4 = px.bar(\n",
    "    df_result_grid,\n",
    "    x=\"rank_test_score\",      \n",
    "    y=\"mean_test_score\",      \n",
    "    color=\"param_solver\",\n",
    "    text=\"rank_test_score\",\n",
    "    hover_data={\n",
    "        \"params_str\": True,           \n",
    "        \"mean_test_score\": True,      \n",
    "        \"rank_test_score\": False,     \n",
    "        \"param_solver\": False,          \n",
    "        \"mts_str\": False             \n",
    "    })\n",
    "fig4.update_layout(\n",
    "    title=\"Distribution des scores par rang avec coloration selon le Solver\",\n",
    "    xaxis_title=\"Rang\",\n",
    "    yaxis_title=\"Mean Test Score\",\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    legend_title='Solver utilisé',\n",
    ")\n",
    "\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation du modèle sur les données d'entrainement\n",
    "print(\"Score calculé par le modèle:\", meta_bscv.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application du modèle aux données de test\n",
    "y_test_pred = meta_bscv.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion sur les données de test prédites\n",
    "cm = confusion_matrix(y_test,y_test_pred)\n",
    "print(cm)\n",
    "df_cm = pd.crosstab(y_test.to_numpy().ravel(), y_test_pred, rownames=['real'], colnames=['predicted'])\n",
    "display(df_cm)\n",
    "\n",
    "# Evaluation du modèle sur les données de test\n",
    "score = sum(cm[i][i] for i in range(0, cm.shape[0]))/cm.sum()\n",
    "print(\"Score reconstruit manuellement:\",score)\n",
    "print(\"Score calculé par le modèle:\", meta_bscv.score(X_test_scaled, y_test))\n",
    "print(\"Rapport de classification complet:\\n\", classification_report(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
