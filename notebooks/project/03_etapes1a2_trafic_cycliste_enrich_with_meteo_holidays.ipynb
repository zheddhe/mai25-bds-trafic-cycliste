{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global\n",
    "import logging\n",
    "from smartcheck.logger_config import setup_logger\n",
    "setup_logger(logging.INFO)\n",
    "\n",
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Machine Learning\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# metrics and evaluation\n",
    "from scipy.stats import anderson, pearsonr, shapiro, normaltest, levene, kruskal\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "### Data Viz\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# graphical missingno\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc\n",
    "import smartcheck.preprocessing_project_specific as pps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2.1 Loading of refactored velo comptage data 2024/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_raw = dfc.load_dataset_from_config('velo_comptage_refactored_data', sep=',', index_col=0)\n",
    "\n",
    "if df_cpt_raw is not None and isinstance(df_cpt_raw, pd.DataFrame):\n",
    "    df_cpt = df_cpt_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.2 Data refactoring and additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = [\n",
    "    \"identifiant_du_compteur\",\n",
    "    \"nom_du_site_de_comptage\",\n",
    "    \"comptage_horaire\",\n",
    "    \"date_et_heure_de_comptage\",\n",
    "    \"orientation_compteur\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"arrondissement\",\n",
    "]\n",
    "import smartcheck.preprocessing_project_specific as pps\n",
    "preprocessor = Pipeline([\n",
    "    (\"filter_columns\", pps.ColumnFilterTransformer(columns_to_keep=keep_cols)),\n",
    "    (\"add_datetime_features\", pps.DatetimePreprocessingTransformer(timestamp_col=\"date_et_heure_de_comptage\")),\n",
    "    (\"add_holiday\", pps.HolidayFromDatetimeTransformer(datetime_col=\"date_et_heure_de_comptage_local\")),\n",
    "    (\"add_school_vacation\", pps.SchoolHolidayTransformer(datetime_col=\"date_et_heure_de_comptage_local\")),\n",
    "    (\"add_weather_data\", pps.WeatherDataEnrichmentTransformer(\n",
    "        lat_col=\"latitude\",\n",
    "        lon_col=\"longitude\",\n",
    "        datetime_col=\"date_et_heure_de_comptage_utc\"\n",
    "    )),\n",
    "    (\"normalize_columns\", pps.ColumnNameNormalizerTransformer()),\n",
    "    (\"add_weather_category\", pps.MeteoCodePreprocessingTransformer(code_col=\"weather_code_wmo_code\")),\n",
    "])\n",
    "\n",
    "df_raw = preprocessor.fit_transform(df_cpt)\n",
    "if df_raw is not None and isinstance(df_raw, pd.DataFrame):\n",
    "    df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Intermediate backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "df_bckp_orig = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df = df_bckp_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 2.3 Explore and verify enriched datas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### General checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infos générales\n",
    "display(df.head())\n",
    "dfc.log_general_info(df)\n",
    "nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df)\n",
    "if nb_first != nb_total:\n",
    "    print(dfc.duplicates_index_map(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Check missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation des valeur NA graphiquement\n",
    "msno.matrix(df_bckp_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Check public holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_bckp_orig[df_bckp_orig.jour_ferie==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Check school holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_dy_vs = df_bckp_orig.groupby(['date_et_heure_de_comptage_day_of_year', 'vacances_scolaires']).identifiant_du_compteur.count().reset_index()\n",
    "display(groupby_dy_vs.head(20))\n",
    "dfc.display_variable_info(df.vacances_scolaires)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Check descriptions and correlation of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_desc_num = df_bckp_orig.select_dtypes(include=np.number).describe()\n",
    "display(df_desc_num)\n",
    "df_desc_cat = df_bckp_orig.select_dtypes(include='object').describe()\n",
    "display(df_desc_cat)\n",
    "df_cr = df_bckp_orig.select_dtypes(include=np.number).corr()\n",
    "display(df_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Analyse statistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderson :\n",
    "# Hypothèse nulle H0 : -> la distribution est normale\n",
    "# Hypothèse alternative H1 : -> on réfute la distribution normale\n",
    "# Explication du test : si pour une tolérance donnée, la statistique de test est supérieure au seuil critique alors on rejette \n",
    "# statistiquement H0 et on accepte H1 sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "\n",
    "# ici tous les seuil même les plus large (15%) rejette l'hypothese de normalité (ce qu'on voyait déjà à l'oeil nu graphiquement)\n",
    "result = anderson(df_bckp_orig['comptage_horaire'])\n",
    "print(f\"Statistique de test = {result.statistic:.4f}\")  # type: ignore\n",
    "for i in range(len(result.critical_values)):  # type: ignore\n",
    "    sig_level = result.significance_level[i]  # type: ignore\n",
    "    crit_value = result.critical_values[i]  # type: ignore\n",
    "    if result.statistic > crit_value:  # type: ignore\n",
    "        print(f\"❌ À {sig_level}% : rejet de la normalité (stat > seuil critique {crit_value:.3f})\")\n",
    "    else:\n",
    "        print(f\"✅ À {sig_level}% : pas de preuve contre la normalité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "#### Pearson (quantitatives against quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson\n",
    "# Hypothèse nulle H0 : -> les deux variables quantitatives ne sont pas correlées (corrélation est nulle)\n",
    "# Hypothèse alternative H1 : -> il existe une correlation (corrélation <> nulle)\n",
    "# Explication du test : si la statistique de test a une p-valeur inférieure à un seuil de tolérance (0,05) alors on rejette \n",
    "# statistiquement H0 et on accepte H1 (il existe une correlation) sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "coeff_corr, p_valeur = pearsonr (x=df_bckp_orig['comptage_horaire'], y=df_bckp_orig['latitude'])\n",
    "print(f\"comptage_horaire/latitude: coefficient de correlation[{coeff_corr}] et p-valeur[{p_valeur}]\")\n",
    "coeff_corr, p_valeur = pearsonr (x=df_bckp_orig['comptage_horaire'], y=df_bckp_orig['longitude'])\n",
    "print(f\"comptage_horaire/longitude: coefficient de correlation[{coeff_corr}] et p-valeur[{p_valeur}]\")\n",
    "\n",
    "corr_matrix = df_bckp_orig.select_dtypes(include='number').corr(method='pearson')\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=np.triu(corr_matrix), center=0)\n",
    "plt.title(\"Matrice de corrélation (Pearson)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### ANOVA (quantitative against qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_cat = [\n",
    "    'arrondissement',\n",
    "    'orientation_compteur',\n",
    "    'nom_du_site_de_comptage',\n",
    "    'weather_code_wmo_code_category',\n",
    "    'vacances_scolaires'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (NB : a priori le test ANOVA n'est pas robuste si notre variable quantitative ne suit pas une loi normale)\n",
    "# Hypothèse nulle H0 : -> pas d'effet significatif de la variable qualitative sur la variable quantitative\n",
    "# Hypothèse alternative H1 : -> il y a un effet significatif de la variable qualitative sur la variable quantitative\n",
    "# Explication du test : si la statistique de test a une p-valeur inférieure a un seuil de tolérance (0,05) alors on rejette \n",
    "# statistiquement H0 et on accepte H1 (il y a un effet significatif de la variable qualitative sur la variable quantitative) \n",
    "# sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "\n",
    "result = smf.ols(f'comptage_horaire ~ {' + '.join(col_cat)}', data=df).fit()\n",
    "display(sm.stats.anova_lm(result))\n",
    "\n",
    "# - Les F-statistiques de l’ANOVA sont élevées → chaque variable apporte significativement à l’explication de la variance de \n",
    "# comptage_horaire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse des coefficients du modèle\n",
    "# R² = 0.285 → le modèle explique 28,5 % de la variance du comptage horaire\n",
    "# beaucoup de p-values de modalités sont très haute (modalités non correlées)\n",
    "# probablement un viol des conditions de l'ANOVA (distribution normale entre les modalités)\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résidus\n",
    "residuals = result.resid\n",
    "\n",
    "# 1.1 Histogramme des résidus\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, kde=True, bins=50)\n",
    "plt.title(\"Distribution des résidus du modèle ANOVA\")\n",
    "plt.xlabel(\"Résidus\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()\n",
    "\n",
    "# 1.2 QQ-plot\n",
    "sm.qqplot(residuals, line='s')\n",
    "plt.title(\"QQ-plot des résidus\")\n",
    "plt.show()\n",
    "\n",
    "# 1.3 Test de normalité de Shapiro (si n <= 5000)\n",
    "if len(residuals) <= 5000:\n",
    "    stat, p = shapiro(residuals)\n",
    "    print(f\"Test de Shapiro-Wilk : p-value = {p:.4f}\")\n",
    "else:\n",
    "    stat, p = normaltest(residuals)\n",
    "    print(f\"Test de D'Agostino : p-value = {p:.4f}\")\n",
    "\n",
    "# 1.4 Test d'homogénéité des variances (Levene)\n",
    "for col in col_cat:\n",
    "    stat_levene, p_levene = levene(*[group[\"comptage_horaire\"].values\n",
    "                                        for _, group in df.groupby(col)])\n",
    "    print(f\"Test de Levene ({col}) : p-value = {p_levene:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Kruskal-Wallis (quantitative against qualitatives, non parametric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les groupes de comptage_horaire par arrondissement\n",
    "for col in col_cat:\n",
    "    grouped = [group[\"comptage_horaire\"].values for _, group in df.groupby(col)]\n",
    "\n",
    "    # Exécute le test de Kruskal-Wallis\n",
    "    stat_kw, p_kw = kruskal(*grouped)\n",
    "    print(f\"Test de Kruskal-Wallis : H = {stat_kw:.2f}, p-value = {p_kw:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.boxplot(x=col, y='comptage_horaire', data=df)\n",
    "    plt.title(f\"Boxplot du comptage horaire par {col}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## 4. Data backup on file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Suppression des colonnes de données périodiques qui doivent être recalculées (allègement du dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    columns=[\n",
    "        'date_et_heure_de_comptage_utc',\n",
    "        'date_et_heure_de_comptage_local',\n",
    "        'date_et_heure_de_comptage_year',\n",
    "        'date_et_heure_de_comptage_month',\n",
    "        'date_et_heure_de_comptage_day',\n",
    "        'date_et_heure_de_comptage_day_of_year',\n",
    "        'date_et_heure_de_comptage_day_of_week',\n",
    "        'date_et_heure_de_comptage_hour',\n",
    "        'date_et_heure_de_comptage_week',\n",
    "        'date_et_heure_de_comptage_dayname',\n",
    "        'date_et_heure_de_comptage_monthname',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "#### Sauvegarde du dataset en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"comptage-velo-donnees-compteurs-2024-2025_Enriched_ML-ready_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
