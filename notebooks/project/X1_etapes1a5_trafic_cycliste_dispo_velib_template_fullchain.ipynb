{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# transformation\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,  TimeSeriesSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# metrics and evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import chi2_contingency, probplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# # graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# # for jupyter notebook display management\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 1.4 General Meta Search functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.meta_search_common as msc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### 2.1.1 VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib_raw = dfc.load_dataset_from_config('velib_dispo_data', sep=';')\n",
    "\n",
    "if df_disp_velib_raw is not None and isinstance(df_disp_velib_raw, pd.DataFrame):\n",
    "    display(df_disp_velib_raw.head())\n",
    "    dfc.log_general_info(df_disp_velib_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_disp_velib_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_disp_velib_raw))\n",
    "    df_disp_velib = dfc.normalize_column_names(df_disp_velib_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib.info()\n",
    "display(df_disp_velib.head())\n",
    "df_cpt_velo_desc = df_disp_velib.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_desc = df_disp_velib.select_dtypes(include='object').describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_disp_velib.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)\n",
    "dfc.display_variable_info(df_disp_velib, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Cross Distribution inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la distribution d'une variable spécique en relation avec les autres de son dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "dfc.analyze_by_reference_variable(df_disp_velib, ref_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée de la distribution d'une variable spécifique en fonction d'autres variables (quantitatives ou qualitatives) du dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "cross_columns = [ref_col] + ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "dfc.log_cross_distributions(\n",
    "    df_disp_velib[cross_columns], \n",
    "    ref_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée d'une variable en fonction d'une autre\n",
    "ref_col = 'borne_de_paiement_disponible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "display(pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée des d'une variable en fonction d'une autre (en conservant les NaN)\n",
    "ref_col = 'station_opening_hours'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "ref_cross_tab = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab)\n",
    "\n",
    "# Catégorisation et normalisation\n",
    "ref_col_val_norm = np.where(\n",
    "    df_disp_velib[ref_col].isin(ref_cross_tab[ref_cross_tab['OUI'] >= 0.8].index.tolist()), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "df_disp_velib[ref_col] = ref_col_val_norm\n",
    "ref_cross_tab_norm = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### Signifiance against target evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification de la signifiance des variables explicatives par rapport à une variable cible\n",
    "ref_col = 'retour_velib_possible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "# Génération des colonnes dummies pour ref_col\n",
    "ref_col_dummies = pd.get_dummies(df_disp_velib[ref_col], prefix=ref_col)\n",
    "print(\"Colonnes dummies générées :\", list(ref_col_dummies.columns))\n",
    "# Pour chaque modalité de cette variable (dummy 0/1), tester sa signifiance avec la variable cible\n",
    "for col in ref_col_dummies:\n",
    "    # Test du Chi-Deux\n",
    "    cross_tab = pd.crosstab(df_disp_velib[target_col], ref_col_dummies[col])\n",
    "    if cross_tab.shape[1] != 2:\n",
    "        print(f\"⚠️ Modalité [{col}] ignorée (1 seule valeur présente)\")\n",
    "        continue\n",
    "    stat, p, _, _ = chi2_contingency(cross_tab)\n",
    "    # V de Cramer\n",
    "    V_Cramer = np.sqrt(\n",
    "        stat/cross_tab.values.sum())\n",
    "    # On affiche uniquement les variables significatives et dont le V de Cramer est supérieur à 0.1\n",
    "    # Faible : Valeur autour de 0.1 ;\n",
    "    # Moyenne : Valeur autour de 0.3 ;\n",
    "    # Elevée : Valeur autour et supérieure à 0.5.\n",
    "    # Lorsque la valeur du V de Cramer est très élevée (aux alentours de 0.8 et plus), on soupçonne généralement de la multicolinéarité.\n",
    "    result = 'significative' if (p < 0.05) and (V_Cramer > 0.1) else 'NON signficative'  # type: ignore\n",
    "    print(f\"Variable [{col}] {result} Vs [{target_col}]: p-value[{p:.5f}], V_Cramer[{V_Cramer:.5f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Backup before modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup before dupplicate management\n",
    "df_disp_velib_orig = df_disp_velib.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Management of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib = df_disp_velib.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation manuelle de la variable cible \n",
    "# /!\\ attention /!\\ les transformation d'encodage meme les plus simples ne sont pas sans fuite de données car elles changent la structure\n",
    "# exemple : un get_dummies qui va créer une colonne sur une valeur rare influence par les colonnes additionnelles (creation de sub features)\n",
    "df_disp_velib.station_en_fonctionnement = df_disp_velib.station_en_fonctionnement.apply(lambda x: 1 if x=='OUI' else 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "for col in df_disp_velib.select_dtypes(include='number').columns:\n",
    "    probplot(df_disp_velib[col], dist=\"norm\", plot=plt)\n",
    "    plt.suptitle(f\"Column {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# 4. Division in Train/Test\n",
    "Lorsqu'il s'agit de données ***chronologiques*** où la tâche consiste à faire des prévisions, les ensembles d'**entraînement**, de **validation** et de **test** doivent être sélectionnés en séparant les données le long de l'axe temporel. C'est-à-dire que les données les plus anciennes sont utilisées pour l'entraînement, les plus récentes pour la validation et les dernières chronologiquement pour les tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temporaire) Ajustement : enlever les variables non retravaillées pour le moment\n",
    "df_disp_velib = df_disp_velib.drop(columns=['identifiant_station',\n",
    "                                            'nom_station', \n",
    "                                            'actualisation_de_la_donnee', \n",
    "                                            'coordonnees_geographiques', \n",
    "                                            'nom_communes_equipees',\n",
    "                                            'code_insee_communes_equipees',\n",
    "                                            'station_opening_hours'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### Division aléatoire simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features (X) et target (y) pour train et test simple\n",
    "target_col = 'station_en_fonctionnement'\n",
    "features = df_disp_velib.drop(target_col, axis=1)\n",
    "target = df_disp_velib[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=66)\n",
    "print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set (X/y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Division temporelle (respect de l'ordre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_col = 'station_en_fonctionnement'\n",
    "# df_disp_velib = df_disp_velib.sort_values(by'actualisation_de_la_donnee')\n",
    "# features = df_disp_velib.drop(target_col, axis=1)\n",
    "# target = df_disp_velib[target_col]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, shuffle=False)\n",
    "# print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "# print(\"Test Set (X/y):\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "Règle d'or : Toute opération qui \"***apprend***\" des données (i.e. utilise l’ensemble des valeurs pour calculer quelque chose) doit être faite après le split train/test — c’est-à-dire uniquement sur le train.\n",
    "\n",
    "| Type de transformation                                                                                    | À faire avant le split ?                    | Détails                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ Création de features basées sur les colonnes existantes (ex: `BMI = weight / height²`)                  | **Avant**                                   | Pas de risque de fuite car c’est purement déterministe.            |\n",
    "| ⚠️ Calculs dépendant de la distribution (moyennes, encodage fréquentiel, imputation par la médiane, etc.) | **Après** (sur le train uniquement)         | Risque de fuite de données si appliqué sur l’ensemble avant split. |\n",
    "| ✅ Ajout de features exogènes fixes (données météo, géographiques, calendaires, etc.)                      | **Avant**                                   | Pas de dépendance au `target` ni à la répartition train/test.      |\n",
    "| ⚠️ Encoding (`LabelEncoder`, `OneHot`, `TargetEncoding`, etc.)                                            | **Fit sur train, transform sur train/test** | Toujours fitter uniquement sur le `train`.                         |\n",
    "| ⚠️ Standardisation / normalisation (Scaler)                                                               | **Fit sur train, transform sur train/test** | Pareil : `.fit()` sur train, `.transform()` sur test.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## 5.1 Modification localisées sur les variables d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple (fictif) de modification localisée des données de test en fonction de la proximité à la médiane d'autre variables\n",
    "# mask = (\n",
    "#     (X_train['Gender'].isna()) &\n",
    "#     (abs(X_train['Age'] - 30) > abs(X_train['Age'] - 41)) & # L’âge est plus proche de 41 que de 30\n",
    "#     (X_train['Previously_Insured'] == 0) & # La personne n’était pas assurée auparavant\n",
    "#     (X_train['Vehicle_Damage'] == 1) # Elle a subi un dommage sur son véhicule\n",
    "# )\n",
    "# X_train.loc[mask, 'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple (fictif) de modification des données de test par répartition spécifique entre deux valeurs 0 et 1 sur 100 (dans une liste)\n",
    "# proportion_tab = [0] * 55 + [1] * 45\n",
    "# mask = (\n",
    "#     (X_train['Gender'].isna())\n",
    "# )\n",
    "# X_train.loc[mask, 'Gender'] = X_train.loc[mask, 'Gender'].apply(lambda x: random.choice(proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing\n",
    " Attention, si le prétraitement des données dépend des étiquettes (supervisé), il doit être fait **séparément pour chaque pli** pour une validation croisée . Sinon, des informations des données de test peuvent influencer l’entraînement, ce qui fausse les résultats en les rendant trop optimistes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### 5.2.1 Scaling (données quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ni outlier ni distribution loi normale : min/max\n",
    "# - sans outlier mais distribution loi normale : standard\n",
    "# - avec outlier : Robust \n",
    "mm_scal = MinMaxScaler()\n",
    "r_scal = RobustScaler()\n",
    "s_scal = StandardScaler()\n",
    "\n",
    "r_scal_col = ['velos_mecaniques_disponibles', 'nombre_total_velos_disponibles']\n",
    "X_train[r_scal_col] = s_scal.fit_transform(X_train[r_scal_col])\n",
    "X_test[r_scal_col] = s_scal.transform(X_test[r_scal_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoding (données qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique                 Type                Colonnes créées     Principe\n",
    "# get_dummies()\t            Nominale\t        N (ou N–1)\t        Binaire par modalité\n",
    "# OneHotEncoder\t            Nominale, Cyclique\tN\t                Colonne 0/1 par modalité\n",
    "# Sum Encoding\t            Nominale\t        N–1\t                Différence avec moyenne globale\n",
    "# Helmert Encoding\t        Nominale\t        N–1\t                Contraste avec moyenne des modalités précédentes\n",
    "# Backward Difference\t    Ordinale\t        N–1\t                Contraste avec moyenne des modalités suivantes\n",
    "# Binary Encoding\t        Nominale\t        log₂(N)\t            Encodage binaire de l’index\n",
    "# Hashing Encoding\t        Nominale\t        n_components\t    Hash des modalités sur colonnes fixes\n",
    "# Label Encoding\t        Ordinale\t        1\t                Entier arbitraire\n",
    "# Ordinal Encoding\t        Ordinale\t        1\t                Rang croissant des modalités\n",
    "# Target Encoding\t        Nominale/Ordinale\t1\t                Moyenne de la cible par modalité\n",
    "# Mean Encoding\t            Nominale/Ordinale\t1\t                Idem Target Encoding\n",
    "# Frequency Encoding\t    Nominale/Ordinale\t1\t                Fréquence d'apparition\n",
    "# Leave-One-Out\t            Nominale/Ordinale\t1\t                Moyenne de la cible, sauf ligne courante\n",
    "# James-Stein Encoding\t    Nominale/Ordinale\t1\t                Moyenne pondérée par variance intercatégorie\n",
    "# M-Estimate Encoding\t    Nominale/Ordinale\t1\t                Moyenne cible lissée vers moyenne globale\n",
    "# Probability Ratio\t        Ordinale, binaire\t1\t                Log du ratio de probas classe 1 / classe 0\n",
    "# WOE Encoding\t            Ordinale, binaire\t1\t                Log( %positif / %négatif )\n",
    "# Thermometer Encoding\t    Ordinale\t        N\t                1 si la modalité est ≤ à une valeur\n",
    "# Trigonométrique (sin/cos)\tCyclique\t        2\t                Encode la cyclicité\n",
    "# Fourier / Radial\t        Cyclique\t        Variable\t        Approximation périodique (base)\n",
    "ohe_enc_col = ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "ohe_enc = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "# Appliquer OneHotEncoder\n",
    "X_train_enc_cat = ohe_enc.fit_transform(X_train[ohe_enc_col])\n",
    "X_test_enc_cat = ohe_enc.transform(X_test[ohe_enc_col])\n",
    "# Ajout des colonnes encodées à un DataFrame car le resultat de enc.fit/transform est un ndarray sans index/colonnes\n",
    "X_train_cat_df = pd.DataFrame(X_train_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_train.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_test.index)  # type: ignore\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train = pd.concat([X_train.drop(columns=ohe_enc_col), X_train_cat_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=ohe_enc_col), X_test_cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "# 6. Selection and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## 6.0 Standard Canvas\n",
    "\n",
    "✅ Résumé des meilleurs préfixes\n",
    "| **Rôle**                            | **Préfixe recommandé** | **Exemples**                                 |\n",
    "| ----------------------------------- | ---------------------- | -------------------------------------------- |\n",
    "| **Classifier**                      | `clf_`                 | `clf_rf`, `clf_svc`, `clf_logreg`, `clf_mlp` |\n",
    "| **Regressor**                       | `reg_`                 | `reg_ridge`, `reg_lgbm`                      |\n",
    "| **Estimator (générique)**           | `est_`                 | `est_model`, `est_transformer`               |\n",
    "| **Meta-estimator / Search**         | `search_`, `meta_`     | `search_gs`, `meta_cv_model`                 |\n",
    "| **Cross-val splitter**              | `cv_`                  | `cv_kfold`, `cv_stratified`                  |\n",
    "| **Feature extractor**               | `fe_`                  | `fe_rbm`, `fe_pca`, `fe_autoencoder`         |\n",
    "| **Pipeline complet (modèle final)** | `pipe_`, `mdl_`        | `pipe_rbm_logreg`, `mdl_cnn_softmax`         |\n",
    "| **Réseau de neurones**              | `nn_`                  | `nn_mnist`, `nn_cnn`, `nn_transformer`       |\n",
    "| **Prétraitement / transformateur**  | `pre_`, `tr_`          | `pre_scaler`, `pre_bin_127`, `tr_encoder`    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "# Hyperparamètres à tester pour chaque modèle\n",
    "param_grids = {\n",
    "    'LogisticRegression': {'C': [0.01, 0.1, 1, 10], 'solver': ['liblinear', 'lbfgs']},\n",
    "    'RandomForestClassifier': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30]},\n",
    "    'SVC': {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "}\n",
    "# Dictionnaire pour stocker les résultats de meta recherche des hyperparamètres\n",
    "results_ms = {}\n",
    "# Exécuter la comparaison pour chaque modèle\n",
    "for model_name, model in models.items():\n",
    "    msc.compare_search_methods(model_name, model, param_grids[model_name], \n",
    "                               X_train, X_test, y_train, y_test, results_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats de meta recherche des hyperparamètres\n",
    "for model_name, model_results in results_ms.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for search_name, search_results in model_results.items():\n",
    "        print(f\"=> {search_name}:\")\n",
    "        print(f\"\\tBest Params: {search_results['best_params']}\")\n",
    "        print(f\"\\tBest CV Score: {search_results['best_cv_score']:2f}\")\n",
    "        print(f\"\\tBest F1 score: {search_results['test_f1_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### NB : la validation croisée peut être assistée avec la fonction cross_val_score (et cross_val_predict également)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les modèles optimisés à tester\n",
    "models_cv = {\n",
    "    'clf_lr' : LogisticRegression(random_state=22, solver='lbfgs'),\n",
    "    'clf_rf' : RandomForestClassifier(random_state=22, n_estimators=200, max_depth=20),\n",
    "    'clf_svc' : SVC(random_state=22, C=1, kernel='rbf')\n",
    "}\n",
    "# Définir le nombre de sous-ensemble et le cross validateur (KFold ici)\n",
    "cv_kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# Stocker les résultats de cross validation pour chaque modèle\n",
    "results_cv = {}\n",
    "# Boucle sur chaque modèle\n",
    "for model_name, model in models_cv.items():\n",
    "    fold_accuracies = []  # Stocke les résultats de précision pour chaque pli (fold)\n",
    "    \n",
    "    # Effectuer la validation croisée sur l'ensemble des données train et test (validation faible) ou juste test (validation forte)\n",
    "    X = pd.concat([X_train, X_test])\n",
    "    y = pd.concat([y_train, y_test])\n",
    "    for train_index, test_index in cv_kf.split(X):\n",
    "        X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]\n",
    "        # display(X_train_cv.head(), X_test_cv.head(), y_train_cv[:5], y_test_cv[:5])\n",
    "        # Entraîner le modèle\n",
    "        model.fit(X_train_cv, y_train_cv)\n",
    "        \n",
    "        # Prédire sur l'ensemble de test\n",
    "        y_test_pred_cv = model.predict(X_test_cv)\n",
    "        \n",
    "        # Calculer la précision\n",
    "        accuracy = accuracy_score(y_test_cv, y_test_pred_cv)\n",
    "        fold_accuracies.append(accuracy)\n",
    "\n",
    "    # Moyenne des résultats de précision pour tous les plis\n",
    "    avg_accuracy = np.mean(fold_accuracies)\n",
    "    var_accuracy = np.var(fold_accuracies, ddof=1)  # ddof=1 pour l'estimation non biaisée (échantillon)\n",
    "\n",
    "    results_cv[model_name] = {\n",
    "        \"mean_accuracy\": avg_accuracy,\n",
    "        \"variance_accuracy\": var_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les résultats\n",
    "for model_name, metrics in results_cv.items():\n",
    "    print(f\"Modèle : {model_name}\")\n",
    "    print(f\"  - Average Accuracy : {metrics['mean_accuracy']:.2f}\")\n",
    "    print(f\"  - Accuracy variance : {metrics['variance_accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## 6.1 Quick Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression configuration\n",
    "logit_config = {\n",
    "    \"target\": \"station_en_fonctionnement\",\n",
    "    \"features\": ['nombre_bornettes_libres', 'nombre_total_velos_disponibles']\n",
    "}\n",
    "\n",
    "# Coefficient adjustment configuration\n",
    "adjustment_config = {\n",
    "    \"nombre_bornettes_libres\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 68)\n",
    "    },\n",
    "    \"nombre_total_velos_disponibles\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 65)\n",
    "    },\n",
    "    # \"nombre_total_velos_disponibles\": {\n",
    "    #     \"type\": \"inverse\"\n",
    "    # },\n",
    "}\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "subset = logit_config[\"features\"]+[logit_config[\"target\"]]\n",
    "cls.logit_analysis(df_train[subset], logit_config, adjustment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "## 6.2 SMOTE/Under/Over Sampling (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## 6.3 Adding threashold (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "## 6.4 Switching to other models (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "# 7. Best model application and evaluation/visualizing of results\n",
    "\n",
    "Rappel pour la classification:\n",
    " - Une **précision** et un **rappel élevé** : La classe a été bien gérée par le modèle.\n",
    " - Une **précision élevée** et un **rappel bas** : La classe n'est pas bien détectée mais lorsqu'elle l'est, le modèle est très fiable.\n",
    " - Une **précision basse** et un **rappel élevé** : La classe est bien détectée, mais inclus également des observations d'autres classes.\n",
    " - Une **précision** et un **rappel bas** : la classe n'a pas du tout été bien gérée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling sur la base d'entraînement\n",
    "smote = SMOTE()\n",
    "X_train_b_smote, y_train_b_smote = smote.fit_resample(X_train, y_train)  # type: ignore\n",
    "\n",
    "# Définition et entraînement du modèle\n",
    "clf_XGB_opti = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_opti.fit(X_train_b_smote, y_train_b_smote)\n",
    "\n",
    "# Prédiction du modèle (Seuil précédemment établi)\n",
    "preds = np.where(clf_XGB_opti.predict_proba(X_test)[:, 1] > 0.026, 1, 0)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Modèle avec rééchantillonnage et optimisation du seuil\")\n",
    "print(\"F1-Score : \", f1_score(preds, y_test))\n",
    "print(confusion_matrix(preds, y_test))\n",
    "\n",
    "# Modèle de base\n",
    "print(\"Modèle de base\")\n",
    "clf_XGB_base = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_base.fit(X_train, y_train)\n",
    "print(\"F1-Score : \", f1_score(clf_XGB_base.predict(X_test), y_test))\n",
    "print(confusion_matrix(clf_XGB_base.predict(X_test), y_test))\n",
    "plot_importance(clf_XGB_base) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
