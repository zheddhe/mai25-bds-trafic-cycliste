{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### general\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "### data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### machine learning (scikit-learn)\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, f_regression, mutual_info_regression, RFE, RFECV\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "\n",
    "### graphical\n",
    "import matplotlib.pyplot as plt\n",
    "# for jupyter notebook management\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 Specific preprocessing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.preprocessing_project_specific as pps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_raw = dfc.load_dataset_from_config('velo_comptage_ml_ready_data', sep=',', index_col=0)\n",
    "\n",
    "if df_cpt_raw is not None and isinstance(df_cpt_raw, pd.DataFrame):\n",
    "    df_cpt = df_cpt_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2.1 Column preprocessing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_change_cols = [\n",
    "    \"identifiant_du_compteur\",\n",
    "    \"comptage_horaire\",\n",
    "    \"orientation_compteur\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"arrondissement\",\n",
    "    \"temperature_2m_c\",\n",
    "    \"rain_mm\",\n",
    "    \"snowfall_cm\",\n",
    "    \"elevation\",\n",
    "]\n",
    "\n",
    "preproc_transf = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"filter\",\n",
    "            pps.ColumnFilterTransformer(columns_to_keep=no_change_cols),\n",
    "            no_change_cols\n",
    "        ),\n",
    "        (\n",
    "            \"datetime\",\n",
    "            pps.DatetimePreprocessingTransformer(timestamp_col=\"date_et_heure_de_comptage\"),\n",
    "            [\"date_et_heure_de_comptage\"]\n",
    "        ),\n",
    "        (\n",
    "            \"meteo_code\",\n",
    "            pps.MeteoCodePreprocessingTransformer(code_col=\"weather_code_wmo_code\"),\n",
    "            [\"weather_code_wmo_code\"]\n",
    "        ),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "preproc_transf.set_output(transform=\"pandas\")\n",
    "\n",
    "df = pd.DataFrame(preproc_transf.fit_transform(df_cpt)) # type: ignore\n",
    "df = df.rename(columns={\n",
    "    col: col.split(\"__\")[-1] for col in df.columns if \"__\" in col\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n",
    "display(df.select_dtypes(include=np.number).describe())\n",
    "display(df.select_dtypes(include='object').describe())\n",
    "display(df.select_dtypes(include=np.number).corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 2.2 Time series processing and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### With Resample (only numeric variables) and seasonal decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_temp = ['date_et_heure_de_comptage_local']\n",
    "col_keep = ['comptage_horaire']\n",
    "df_res = df[col_temp+col_keep].set_index(col_temp)\n",
    "# resample sur l'index\n",
    "df_res = df_res.resample('h').sum()\n",
    "display(df_res.info())\n",
    "display(df_res.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_res = seasonal_decompose(df_res[:24*30], model='additive', period=24)\n",
    "fig = decomp_res.plot()\n",
    "fig.set_figwidth(12)\n",
    "fig.set_figheight(12)\n",
    "plt.show()\n",
    "# Ce graphique permet de préestimer d, D, et s dans le modèle SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### With GroupBy and Periods (categorical and numerical variables) and ACF/PACF decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition des variables utiles (temporelle/numeriques/categorielles)\n",
    "col_temp = ['date_et_heure_de_comptage_local']\n",
    "num_col_keep = list(df.select_dtypes(include=np.number).columns)\n",
    "cat_col_keep = [\n",
    "    'weather_code_wmo_code_category',\n",
    "    'orientation_compteur',\n",
    "    'arrondissement'\n",
    "]\n",
    "# extraction des variables pertinentes et reindexation via la variable temporelle\n",
    "df_gpd = df[col_temp+num_col_keep+cat_col_keep].copy().set_index(col_temp)\n",
    "# création d'une colonne périodique \n",
    "df_gpd['hour_period'] = df_gpd.index.to_period('h') # type: ignore\n",
    "# cumuls pour les variables numériques par période\n",
    "df_grby_cum = df_gpd.groupby('hour_period')[num_col_keep].sum()\n",
    "# proportions pour les variables categorielles par période\n",
    "for cat in cat_col_keep:\n",
    "    cat_grby_cnt = df_gpd.groupby(['hour_period']+[cat]).size().unstack(fill_value=0)\n",
    "    cat_grby_props = cat_grby_cnt.div(cat_grby_cnt.sum(axis=1), axis=0)\n",
    "    df_grby_cum = df_grby_cum.join(cat_grby_props)\n",
    "df_gpd = df_grby_cum.copy()\n",
    "display(df_grby_cum.info())\n",
    "display(df_grby_cum.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Matrice de correlation des variables numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))  # Taille de la figure\n",
    "corr_mat = df_grby_cum.corr()\n",
    "sns.heatmap(corr_mat, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=np.triu(corr_mat))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des autocorrelations successives avec visualisation du résidu\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(15,25))\n",
    "# NB : la série est déjà stationnaire donc pas de différentiation simple nécessaire\n",
    "y = df_gpd['comptage_horaire'][:3000]\n",
    "# différenciation saisonnière 1 (saisonalité sur 24h intuitée)\n",
    "s1 = 24\n",
    "# différenciation saisonnière 2 (sous saisonalité sur 48h intuitée)\n",
    "s2 = 24*7\n",
    "\n",
    "y.plot(ax=ax1)\n",
    "ax1.set_title('Données brutes')\n",
    "pd.plotting.autocorrelation_plot(y, ax=ax2)\n",
    "ax2.set_title(f'Autocorrelation données brutes')\n",
    "\n",
    "# 1ère différenciation\n",
    "y_s1 = y.diff(s1).dropna()\n",
    "y_s1.plot(ax=ax3)\n",
    "ax3.set_title(f'Données résidus après 1ère Différenciation {s1}h')\n",
    "pd.plotting.autocorrelation_plot(y_s1, ax=ax4)\n",
    "ax4.set_title(f'Autocorrelation après 1ère Différenciation {s1}h')\n",
    "\n",
    "# 2ème différenciation\n",
    "y_s2 = y_s1.diff(s2).dropna()\n",
    "y_s2.plot(ax=ax5)\n",
    "ax5.set_title(f'Données résidus après 2ème Différenciation {s2}h')\n",
    "pd.plotting.autocorrelation_plot(y_s2, ax=ax6)\n",
    "ax6.set_title(f'Autocorrelation après 2ème Différenciation {s2}h')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test statistique de la stationnarité\n",
    "result = adfuller(y)\n",
    "print(\"Stationnarité sans différentiation\",result[1]) \n",
    "result  = adfuller(y_s1)\n",
    "print(\"Stationnarité après 1ère différenciation\",result[1]) \n",
    "result = adfuller(y_s2)\n",
    "print(\"Stationnarité après 2nde différenciation\",result[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features / target separation\n",
    "features = df_gpd.drop(columns=['comptage_horaire'])\n",
    "target = df_gpd['comptage_horaire']\n",
    "test_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test separation \n",
    "index_ratio = int(df_gpd.shape[0]*test_ratio//1)\n",
    "X_train = features.iloc[:-index_ratio].copy()\n",
    "X_test = features.iloc[-index_ratio:].copy()\n",
    "y_train = target.iloc[:-index_ratio].copy()\n",
    "y_test = target.iloc[-index_ratio:].copy()\n",
    "\n",
    "display(X_train.shape, y_train.shape)\n",
    "display(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation des échelles\n",
    "# Preprocessing des variables explicatives d'entrainement et de test (scaler)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "# Recuperation des propriétés du dataframe perdues (nparray) avec le scaler\n",
    "X_train = pd.DataFrame(s_scaler.fit_transform(X_train), index=X_train.index)\n",
    "X_test = pd.DataFrame(s_scaler.transform(X_test), index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 3.1 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### 3.1.1 KBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_skb_mir = SelectKBest(score_func = mutual_info_regression, k=13)\n",
    "fe_skb_mir.fit(X_train, y_train)\n",
    "mask = fe_skb_mir.get_support()\n",
    "plt.matshow(mask.reshape(1,-1), cmap = 'gray_r')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlabel('Axe des features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_skb_fr = SelectKBest(score_func = f_regression, k=13)\n",
    "fe_skb_fr.fit(X_train, y_train)\n",
    "mask = fe_skb_fr.get_support()\n",
    "plt.matshow(mask.reshape(1,-1), cmap = 'gray_r')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlabel('Axe des features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### 3.1.2 RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LinearRegression()\n",
    "fe_rfe = RFE(estimator=clf_lr, step=1, n_features_to_select = 13)\n",
    "fe_rfe.fit(X_train, y_train)\n",
    "mask = fe_rfe.get_support()\n",
    "plt.matshow(mask.reshape(1,-1), cmap = 'gray_r')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlabel('Axe des features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking = fe_rfe.ranking_\n",
    "print(ranking)\n",
    "plt.matshow(ranking.reshape(1,-1), cmap = 'gray')\n",
    "plt.xlabel('Axe des features')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### 3.1.3 RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_kf = TimeSeriesSplit(n_splits=5)\n",
    "clf_lr = LinearRegression()\n",
    "fe_rfecv = RFECV(estimator=clf_lr, cv = cv_kf, step=1)\n",
    "fe_rfecv.fit(X_train, y_train)\n",
    "mask = fe_rfecv.get_support()\n",
    "plt.matshow(mask.reshape(1,-1), cmap = 'gray_r')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlabel('Axe des features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "for i in range(5):\n",
    "    ax1.plot(fe_rfecv.cv_results_[f'split{i}_test_score'])\n",
    "ax1.set_xlabel('Nombre de features')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Score par fold de test pour chaque itération')\n",
    "ax2.plot(fe_rfecv.cv_results_['mean_test_score'])\n",
    "ax2.set_xlabel('Nombre de features')\n",
    "ax2.set_ylabel('Score')\n",
    "ax2.set_title('Score moyen en cross validation')\n",
    "plt.show()\n",
    "print(\"Nombre de features retenues :\", fe_rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### 3.1.4 LASSO + GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_kf = TimeSeriesSplit(n_splits=5)\n",
    "clf_lasso = Lasso(alpha = 1, max_iter=5000)\n",
    "alpha_grid = {'alpha':[1/i for i in range(1,10)]}\n",
    "search_gscv = GridSearchCV(estimator = clf_lasso, param_grid = alpha_grid, cv=cv_kf, scoring = 'neg_mean_squared_error')\n",
    "search_gscv.fit(X_train, y_train)\n",
    "best_clf_lasso = search_gscv.best_estimator_\n",
    "print(\"Meilleur alpha :\", search_gscv.best_params_)\n",
    "fe_sfm = SelectFromModel(estimator=best_clf_lasso, threshold=1e-10, prefit=True)\n",
    "mask = fe_sfm.get_support()\n",
    "plt.matshow(mask.reshape(1,-1), cmap = 'gray_r')\n",
    "plt.xticks(ticks=range(X_train.shape[1]), labels=X_train.columns, rotation=90)\n",
    "plt.yticks([])\n",
    "plt.xlabel('Axe des features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 3.2 Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 3.2.1 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# récuperation des coordonnées de primary components par défaut\n",
    "fe_pca = PCA()\n",
    "coord_pca_train = fe_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "#### interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique de la part de variance expliquée par composante \n",
    "fig_pca, (ax1_pca, ax2_pca) = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax1_pca.set_xlim(0, 30)\n",
    "ax1_pca.set_xticks(range(30))\n",
    "ax1_pca.set_xlabel('Nombre de composantes')\n",
    "ax1_pca.set_ylabel('Part de variance expliquée')\n",
    "ax1_pca.plot(fe_pca.explained_variance_ratio_)\n",
    "# Graphique de la part de variance expliquée cumulée avec seuil de 90%\n",
    "ax2_pca.set_xlim(0, 30)\n",
    "ax2_pca.set_xticks(range(30))\n",
    "ax2_pca.set_xlabel('Nombre de composantes')\n",
    "ax2_pca.set_ylabel('Variance expliquée cumulée')\n",
    "ax2_pca.plot(fe_pca.explained_variance_ratio_.cumsum())\n",
    "ax2_pca.axhline(y=0.9, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Graphique de la part de variance des X composantes principales par rapport au reste des composantes\n",
    "x = 6\n",
    "pca_var_ratios = list(fe_pca.explained_variance_ratio_[:x])\n",
    "pca_var_ratios.append(sum(fe_pca.explained_variance_ratio_[x:]))\n",
    "\n",
    "plt.pie(\n",
    "    pca_var_ratios, \n",
    "    labels=[f'PC{i}' for i in range(x)]+['Autres PCx'], \n",
    "    autopct='%1.2f%%'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Feature projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection sur le premier axe de la composante 0\n",
    "component_0 = fe_pca.components_[0,:]\n",
    "explained_var_0 = fe_pca.explained_variance_[0]\n",
    "corr_axe0 = component_0 * np.sqrt(explained_var_0)\n",
    "display(corr_axe0.shape)\n",
    "\n",
    "# projection sur le second axe de la composante 1\n",
    "component_1 = fe_pca.components_[1,:]\n",
    "explained_var_1 = fe_pca.explained_variance_[1]\n",
    "corr_axe1 = component_1 * np.sqrt(explained_var_1)\n",
    "display(corr_axe1.shape)\n",
    "\n",
    "# creation d'un dataset avec les deux axes\n",
    "charges_factorielles = pd.DataFrame(\n",
    "    [corr_axe0, corr_axe1],\n",
    "    columns=features.columns,\n",
    "    index=['Axe 0', 'Axe 1'])\n",
    "display(charges_factorielles)\n",
    "\n",
    "def draw_correlation_circle(df_charges_factorielles, pca, arrow_length=0.01, label_rotation=0):\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    for i, var in enumerate(df_charges_factorielles.columns):\n",
    "        x = df_charges_factorielles.loc['Axe 0', var]\n",
    "        y = df_charges_factorielles.loc['Axe 1', var]\n",
    "        ax.arrow(0, 0, \n",
    "                 x*(1-arrow_length), \n",
    "                 y*(1-arrow_length), \n",
    "                 head_width=arrow_length, \n",
    "                 head_length=arrow_length, alpha=0.5, fc='blue', ec='blue')\n",
    "        ax.text(x, y, var,\n",
    "                ha='center', va='center',\n",
    "                fontsize=9, rotation=label_rotation, clip_on=True)\n",
    "    circle = plt.Circle((0, 0), 1, facecolor='none', edgecolor=\"#1EE4C9\")\n",
    "    ax.add_artist(circle)\n",
    "    ax.set_xlim(-1.1, 1.1)\n",
    "    ax.set_ylim(-1.1, 1.1)\n",
    "    ax.set_aspect('equal', adjustable='box')\n",
    "    ax.set_xlabel('Axe 0 (PC0)')\n",
    "    ax.set_ylabel('Axe 1 (PC1)')\n",
    "    ax.set_title('Cercle des Corrélations')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "# Tracer le cercle de corrélation\n",
    "draw_correlation_circle(charges_factorielles, fe_pca, arrow_length=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### Features data (coordinates) projection and interpretation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(coord_pca_train.shape)\n",
    "\n",
    "df_plot = pd.DataFrame(coord_pca_train[:150, :2], columns=[\"PC0\", \"PC1\"])  \n",
    "df_plot[\"target\"] = pd.Series(y_train[:150]).reset_index(drop=True)\n",
    "\n",
    "# Affichage du graphique de projection des coordonnées sur les deux axes principaux (seaborn)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    data=df_plot,\n",
    "    x='PC0', y='PC1',\n",
    "    hue='target',           # coloration selon la variable cible\n",
    "    palette='coolwarm',         # ou 'viridis', 'coolwarm', etc.\n",
    "    s=30, edgecolor='k'\n",
    ")\n",
    "plt.title(\"Projection PCA - composantes 0 et 1 pour 150 échantillons\")\n",
    "plt.xlabel(\"PC0\")\n",
    "plt.ylabel(\"PC1\")\n",
    "plt.legend(title='Cible')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Affichage du graphique de projection des coordonnées sur les deux axes principaux (matplotlib)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(df_plot.PC0, df_plot.PC1,  c = df_plot.target, cmap=plt.cm.coolwarm, alpha = .7, s = 30)\n",
    "ax.set_title(\"Données projetées sur les 2 composantes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## 4.1 Linear Regression (with manual AR1-MA1 dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features / target separation\n",
    "features = df_res.drop(columns='comptage_horaire')\n",
    "target = df_res['comptage_horaire']\n",
    "test_ratio = 0.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test separation avec ajout colonne\n",
    "index_ratio = int(df_res.shape[0]*test_ratio//1)\n",
    "X_train = features.iloc[:-index_ratio].copy()\n",
    "X_test = features.iloc[-index_ratio:].copy()\n",
    "y_train = target.iloc[:-index_ratio].copy()\n",
    "y_test = target.iloc[-index_ratio:].copy()\n",
    "# Création de la moyenne glissante (24 autour de t) et de la valeur t-1\n",
    "X_train['comptage_horaire_lag1'] = y_train.shift(1)\n",
    "X_train['comptage_horaire_MA24'] = y_train.rolling(window=24).mean()\n",
    "mask_train = X_train.notna().all(axis=1)\n",
    "X_train = X_train.loc[mask_train]\n",
    "y_train = y_train.loc[mask_train]\n",
    "X_test['comptage_horaire_lag1'] = y_test.shift(1)\n",
    "X_test['comptage_horaire_MA24'] = y_test.rolling(window=24).mean()\n",
    "mask_train = X_test.notna().all(axis=1)\n",
    "X_test = X_test.loc[mask_train]\n",
    "y_test = y_test.loc[mask_train]\n",
    "\n",
    "display(X_train.shape, y_train.shape)\n",
    "display(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "# Preprocessing des variables explicatives d'entrainement et de test (scaler)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "# Recuperation des propriétés du dataframe perdues (nparray) avec le scaler\n",
    "X_train = pd.DataFrame(s_scaler.fit_transform(X_train), index=X_train.index)\n",
    "X_test = pd.DataFrame(s_scaler.transform(X_test), index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_lr = LinearRegression()\n",
    "clf_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = clf_lr.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_test_pred)\n",
    "print(f'Erreur quadratique moyenne (RMSE) : {rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(y_test.index, y_test, label='Valeurs réelles')  # type: ignore\n",
    "plt.plot(y_test.index, y_test_pred, label='Prédictions', linestyle='--')  # type: ignore\n",
    "plt.title('Prédictions de la régression linéaire')\n",
    "plt.xlabel('Date')\n",
    "plt.xlim([pd.to_datetime('2025-04-01'), pd.to_datetime('2025-04-16')])\n",
    "plt.ylabel('Comptage Horaire')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## 4.2 ARIMA / SARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Features / target separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features / target separation\n",
    "features = df_gpd.drop(columns=['comptage_horaire'])\n",
    "target = df_gpd['comptage_horaire']\n",
    "test_ratio = 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Train / test separation (temporal context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train / test separation \n",
    "index_ratio = int(df_gpd.shape[0]*test_ratio//1)\n",
    "X_train = features.iloc[:-index_ratio].copy()\n",
    "X_test = features.iloc[-index_ratio:].copy()\n",
    "y_train = target.iloc[:-index_ratio].copy()\n",
    "y_test = target.iloc[-index_ratio:].copy()\n",
    "\n",
    "display(X_train.shape, y_train.shape)\n",
    "display(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation des échelles\n",
    "# Preprocessing des variables explicatives d'entrainement et de test (scaler)\n",
    "s_scaler = StandardScaler().fit(X_train)\n",
    "# Recuperation des propriétés du dataframe perdues (nparray) avec le scaler\n",
    "X_train = pd.DataFrame(s_scaler.fit_transform(X_train), index=X_train.index)\n",
    "X_test = pd.DataFrame(s_scaler.transform(X_test), index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Analyse de la saisonnalité par ACF/PCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paramètres de saisonnalité\n",
    "max_lag1 = s1*4\n",
    "seasonal_lags1 = [k * s1 for k in range(1, (max_lag1 // s1) + 1)]\n",
    "max_lag2 = s2*4\n",
    "seasonal_lags2 = [k * s2 for k in range(1, (max_lag2 // s2) + 1)]\n",
    "\n",
    "def ajouter_lignes_saisonnieres(ax, seasonal_lags):\n",
    "    for lag in seasonal_lags:\n",
    "        ax.axvline(x=lag, color='red', linestyle='--', alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pré visualisation de :\n",
    "# - l'autocorrelation simple (ACF - basé sur les X précédentes observations) \n",
    "# - l'autocorrelation partielle (PACF basée sur les moyennes mobiles d'amplitude X)\n",
    "# Ces éléments permettent de pré-identifier les hypoerparamètres de SARIMA / ARIMA\n",
    "# - d/D selon si les ACF et PACF tendent vers 0 ou stoppent net (ARMA si les deux tendent vers 0)\n",
    "# - p/q grâce au 1er lag ACF/PACF entrant dans la zone statistiquement peu probable\n",
    "# - P/Q grâce au 1er pic de lag cyclique (à motif saisonnier) ACF/PACF entrant dans la première zone statistiquement peu probable\n",
    "fig2, (\n",
    "    (ax21, ax22), \n",
    "    (ax23, ax24), \n",
    "    (ax25, ax26)\n",
    ") = plt.subplots(3, 2, figsize=(20,20))\n",
    "\n",
    "plot_acf(y, lags = max_lag1, ax=ax21)\n",
    "ax21.set_title(f'ACF sans différenciation')\n",
    "plot_pacf(y, lags = max_lag1, ax=ax22)\n",
    "ax22.set_title(f'PACF sans différenciation')\n",
    "plot_acf(y_s1, lags = max_lag1, ax=ax23)\n",
    "ajouter_lignes_saisonnieres(ax23, seasonal_lags1)\n",
    "ax23.set_title(f'ACF avec 1ère différenciation {s1} lags (heures)')\n",
    "plot_pacf(y_s1, lags = max_lag1, ax=ax24)\n",
    "ajouter_lignes_saisonnieres(ax24, seasonal_lags1)\n",
    "ax24.set_title(f'PACF avec 1ère différenciation {s1} lags (heures)')\n",
    "plot_acf(y_s2, lags = max_lag2, ax=ax25)\n",
    "ajouter_lignes_saisonnieres(ax25, seasonal_lags2)\n",
    "ax25.set_title(f'ACF avec 2ème différenciation {s2} lags (heures)')\n",
    "plot_pacf(y_s2, lags = max_lag2, ax=ax26)\n",
    "ajouter_lignes_saisonnieres(ax26, seasonal_lags2)\n",
    "ax26.set_title(f'PACF avec 2ème différenciation {s2} lags (heures)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    ">NB: Pour une double saisonalité passer a des modèles plus robustes comme TBATS ou BATS ou alors Fourier terms + ARIMA\n",
    ">\n",
    ">NB2: les différenciation exploratoires initiales de saisonalité sont la pour donner un indice il faut appliquer les modèles sur les données non différenciées néanmoins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_sarima = SARIMAX(y_train, order=(1,1,1), seasonal_order=(1,1,1,24)) \n",
    "result_sarima = clf_sarima.fit()\n",
    "print(result_sarima.summary())  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = len(y_test)\n",
    "assert X_test.shape[0] == n_test\n",
    "y_test_pred = result_sarima.get_forecast(  # type: ignore    \n",
    "    steps=n_test, \n",
    ").summary_frame()\n",
    "display(y_test_pred)\n",
    "fig_sarima, ax_sarima = plt.subplots(figsize = (15,5))\n",
    "plt.plot(y_test.index.to_timestamp(), y_test, label='Valeurs réelles')  # type: ignore\n",
    "plt.plot(y_test_pred[\"mean\"].index.to_timestamp(), y_test_pred[\"mean\"], label='Prédictions', linestyle='--')\n",
    "# ax_sarima.fill_between(y_test_pred.index, y_test_pred['mean_ci_lower'], y_test_pred['mean_ci_upper'], color='orange', alpha=0.1);\n",
    "# # Ligne de démarcation du début des forecasts\n",
    "# plt.axvline(x= datetime.date(2025,4,15), color='red')\n",
    "plt.title('Prédictions de SARIMA')\n",
    "plt.xlabel('Date')\n",
    "plt.xlim([pd.to_datetime('2025-04-01'), pd.to_datetime('2025-04-16')])\n",
    "plt.ylabel('Comptage Horaire')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Coute environ 12 minutes de CPU\n",
    "# clf_sarimax = SARIMAX(y_train[-2000:], X_train[-2000:], order=(3,1,4),seasonal_order=(1,1,1,24))\n",
    "# result_sarimax=clf_sarimax.fit()\n",
    "# print(result_sarimax.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sauvegarde avec pickle\n",
    "# with open(\"sarimax_model.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(result_sarimax, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chargement avec pickle depuis le modèle enregistré précédemment (si pas réentrainé)\n",
    "# with open(\"sarimax_model.pkl\", \"rb\") as f:\n",
    "#     result_sarimax = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_test = len(y_test)\n",
    "# assert X_test.shape[0] == n_test\n",
    "# y_test_pred = result_sarimax.get_forecast(    \n",
    "#     steps=n_test, \n",
    "#     exog=X_test\n",
    "# ).summary_frame()\n",
    "# display(y_test_pred)\n",
    "# fig_sarimax, ax_sarimax = plt.subplots(figsize = (15,5))\n",
    "# plt.plot(y_test.index.to_timestamp(), y_test, label='Valeurs réelles')  # type: ignore\n",
    "# plt.plot(y_test_pred['mean'].index.to_timestamp(), y_test_pred['mean'], label='Prédictions', linestyle='--')\n",
    "# ax_sarimax.fill_between(y_test_pred.index, y_test_pred['mean_ci_lower'], y_test_pred['mean_ci_upper'], color='orange', alpha=0.1)\n",
    "# # # Ligne de démarcation du début des forecasts\n",
    "# # plt.axvline(x= datetime.date(2025,4,15), color='red')\n",
    "# plt.title('Prédictions de SARIMA avec variables exogènes (X)')\n",
    "# plt.xlabel('Date')\n",
    "# plt.xlim([pd.to_datetime('2025-04-01'), pd.to_datetime('2025-04-16')])\n",
    "# plt.ylabel('Comptage Horaire')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
