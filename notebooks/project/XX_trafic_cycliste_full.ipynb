{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73fb7a13ed0fb62",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c542ef7e7fc530b",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d21014e831ae4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T12:10:30.601532Z",
     "start_time": "2025-05-23T12:10:30.598509Z"
    }
   },
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# transformation\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# metrics and evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from scipy.stats import chi2_contingency, probplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# # graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# # for jupyter notebook display management\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e380e5d201d38b7",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c8614",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2796587189d32955",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3cc3451038dac",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17618b",
   "metadata": {},
   "source": [
    "### 2.1.1 VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec40b1e",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo_raw = dfc.load_dataset_from_config('velo_comptage_data', sep=';')\n",
    "\n",
    "if df_cpt_velo_raw is not None and isinstance(df_cpt_velo_raw, pd.DataFrame):\n",
    "    display(df_cpt_velo_raw.head())\n",
    "    dfc.log_general_info(df_cpt_velo_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_cpt_velo_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_cpt_velo_raw))\n",
    "    df_cpt_velo = dfc.normalize_column_names(df_cpt_velo_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f75cf91",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.info()\n",
    "display(df_cpt_velo.head())\n",
    "df_cpt_velo_desc = df_cpt_velo.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_cpt_velo.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dea908",
   "metadata": {},
   "source": [
    "### 2.1.2 VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48c5a6",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aca70ecd678b88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T12:10:34.515566Z",
     "start_time": "2025-05-23T12:10:31.485324Z"
    }
   },
   "outputs": [],
   "source": [
    "df_disp_velib_raw = dfc.load_dataset_from_config('velib_dispo_data', sep=';')\n",
    "\n",
    "if df_disp_velib_raw is not None and isinstance(df_disp_velib_raw, pd.DataFrame):\n",
    "    display(df_disp_velib_raw.head())\n",
    "    dfc.log_general_info(df_disp_velib_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_disp_velib_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_disp_velib_raw))\n",
    "    df_disp_velib = dfc.normalize_column_names(df_disp_velib_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3f37a",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de03f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib.info()\n",
    "display(df_disp_velib.head())\n",
    "df_cpt_velo_desc = df_disp_velib.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_disp_velib.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)\n",
    "dfc.display_variable_info(df_disp_velib, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d79a5e",
   "metadata": {},
   "source": [
    "#### Cross Distribution inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994f6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la distribution d'une variable spécique en relation avec les autres de son dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "dfc.analyze_by_reference_variable(df_disp_velib, ref_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303e19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée de la distribution d'une variable spécifique en fonction d'autres variables (quantitatives ou qualitatives) du dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "cross_columns = [ref_col] + ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "dfc.log_cross_distributions(\n",
    "    df_disp_velib[cross_columns], \n",
    "    ref_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée d'une variable en fonction d'une autre\n",
    "ref_col = 'borne_de_paiement_disponible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "display(pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d4ee6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée des d'une variable en fonction d'une autre (en conservant les NaN)\n",
    "ref_col = 'station_opening_hours'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "ref_cross_tab = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab)\n",
    "\n",
    "# Catégorisation et normalisation\n",
    "ref_col_val_norm = np.where(\n",
    "    df_disp_velib[ref_col].isin(ref_cross_tab[ref_cross_tab['OUI'] >= 0.8].index.tolist()), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "df_disp_velib[ref_col] = ref_col_val_norm\n",
    "ref_cross_tab_norm = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a55d",
   "metadata": {},
   "source": [
    "#### Signifiance against target evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31299cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification de la signifiance des variables explicatives par rapport à une variable cible\n",
    "ref_col = 'retour_velib_possible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "# Génération des colonnes dummies pour ref_col\n",
    "ref_col_dummies = pd.get_dummies(df_disp_velib[ref_col], prefix=ref_col)\n",
    "print(\"Colonnes dummies générées :\", list(ref_col_dummies.columns))\n",
    "# Pour chaque modalité de cette variable (dummy 0/1), tester sa signifiance avec la variable cible\n",
    "for col in ref_col_dummies:\n",
    "    # Test du Chi-Deux\n",
    "    cross_tab = pd.crosstab(df_disp_velib[target_col], ref_col_dummies[col])\n",
    "    if cross_tab.shape[1] != 2:\n",
    "        print(f\"⚠️ Modalité [{col}] ignorée (1 seule valeur présente)\")\n",
    "        continue\n",
    "    stat, p, _, _ = chi2_contingency(cross_tab)\n",
    "    # V de Cramer\n",
    "    V_Cramer = np.sqrt(\n",
    "        stat/cross_tab.values.sum())\n",
    "    # On affiche uniquement les variables significatives et dont le V de Cramer est supérieur à 0.1\n",
    "    # Faible : Valeur autour de 0.1 ;\n",
    "    # Moyenne : Valeur autour de 0.3 ;\n",
    "    # Elevée : Valeur autour et supérieure à 0.5.\n",
    "    # Lorsque la valeur du V de Cramer est très élevée (aux alentours de 0.8 et plus), on soupçonne généralement de la multicolinéarité.\n",
    "    result = 'significative' if (p < 0.05) and (V_Cramer > 0.1) else 'NON signficative'\n",
    "    print(f\"Variable [{col}] {result} Vs [{target_col}]: p-value[{p:.5f}], V_Cramer[{V_Cramer:.5f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a26968",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532a0cc",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d269798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and duplicates management\n",
    "df_cpt_velib_orig = df_cpt_velo.copy()\n",
    "df_cpt_velo = df_cpt_velo.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f127a27",
   "metadata": {},
   "source": [
    "### 2.2.2 VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ec553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and duplicates management\n",
    "df_disp_velib_orig = df_disp_velib.copy()\n",
    "df_disp_velib = df_disp_velib.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation manuelle de la variable cible \n",
    "# /!\\ attention /!\\ les transformation d'encodage meme les plus simples ne sont pas sans fuite de données car elles changent la structure\n",
    "# exemple : un get_dummies qui va créer une colonne sur une valeur rare influence par les colonnes additionnelles (creation de sub features)\n",
    "df_disp_velib.station_en_fonctionnement = df_disp_velib.station_en_fonctionnement.apply(lambda x: 1 if x=='OUI' else 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44e0f9",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a9476",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6035ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "for col in df_disp_velib.select_dtypes(include='number').columns:\n",
    "    probplot(df_disp_velib[col], dist=\"norm\", plot=plt)\n",
    "    plt.suptitle(f\"Column {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba447f0a",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9c226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e229c390",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a072e94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fb35015",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702e1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecb51d68",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652dbafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff7fd865",
   "metadata": {},
   "source": [
    "# 4. Division in Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f343cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temporaire) Ajustement : enlever les variables non retravaillées pour le moment\n",
    "df_disp_velib = df_disp_velib.drop(columns=['identifiant_station',\n",
    "                                            'nom_station', \n",
    "                                            'actualisation_de_la_donnee', \n",
    "                                            'coordonnees_geographiques', \n",
    "                                            'nom_communes_equipees',\n",
    "                                            'code_insee_communes_equipees',\n",
    "                                            'station_opening_hours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ac988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features (X) et target (y) pour train et test\n",
    "target_col = 'station_en_fonctionnement'\n",
    "features = df_disp_velib.drop(target_col, axis=1)\n",
    "target = df_disp_velib[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=66)\n",
    "print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set (X/y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3727a13",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "Règle d'or : Toute opération qui \"apprend\" des données (i.e. utilise l’ensemble des valeurs pour calculer quelque chose) doit être faite après le split train/test — c’est-à-dire uniquement sur le train.\n",
    "\n",
    "| Type de transformation                                                                                    | À faire avant le split ?                    | Détails                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ Création de features basées sur les colonnes existantes (ex: `BMI = weight / height²`)                  | **Avant**                                   | Pas de risque de fuite car c’est purement déterministe.            |\n",
    "| ⚠️ Calculs dépendant de la distribution (moyennes, encodage fréquentiel, imputation par la médiane, etc.) | **Après** (sur le train uniquement)         | Risque de fuite de données si appliqué sur l’ensemble avant split. |\n",
    "| ✅ Ajout de features exogènes fixes (données météo, géographiques, calendaires, etc.)                      | **Avant**                                   | Pas de dépendance au `target` ni à la répartition train/test.      |\n",
    "| ⚠️ Encoding (`LabelEncoder`, `OneHot`, `TargetEncoding`, etc.)                                            | **Fit sur train, transform sur train/test** | Toujours fitter uniquement sur le `train`.                         |\n",
    "| ⚠️ Standardisation / normalisation (Scaler)                                                               | **Fit sur train, transform sur train/test** | Pareil : `.fit()` sur train, `.transform()` sur test.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950b779",
   "metadata": {},
   "source": [
    "## 5.1 Modification localisées sur les variables d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b78c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification localisée en fonction de la proximité à la médiane d'autre variables\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "#     (abs(train['Age'] - 30) > abs(train['Age'] - 41)) & # L’âge est plus proche de 41 que de 30\n",
    "#     (train['Previously_Insured'] == 0) & # La personne n’était pas assurée auparavant\n",
    "#     (train['Vehicle_Damage'] == 1) # Elle a subi un dommage sur son véhicule\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122363d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification par répartition spécifique entre deux valeurs 0 et 1\n",
    "# proportion_tab = [0] * 55 + [1] * 45\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = train.loc[mask, 'Gender'].apply(lambda x: random.choice(proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a97e1b7",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429c5bc",
   "metadata": {},
   "source": [
    "### 5.2.1 Scaling (données quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8f18ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ni outlier ni distribution loi normale : min/max\n",
    "# - sans outlier mais distribution loi normale : standard\n",
    "# - avec outlier : Robust \n",
    "mm_scal = MinMaxScaler()\n",
    "r_scal = RobustScaler()\n",
    "s_scal = StandardScaler()\n",
    "\n",
    "r_scal_col = ['velos_mecaniques_disponibles', 'nombre_total_velos_disponibles']\n",
    "df_disp_velib[r_scal_col] = s_scal.fit_transform(df_disp_velib[r_scal_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90095c9",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoding (données qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2365ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique                 Type                Colonnes créées     Principe\n",
    "# get_dummies()\t            Nominale\t        N (ou N–1)\t        Binaire par modalité\n",
    "# OneHotEncoder\t            Nominale, Cyclique\tN\t                Colonne 0/1 par modalité\n",
    "# Sum Encoding\t            Nominale\t        N–1\t                Différence avec moyenne globale\n",
    "# Helmert Encoding\t        Nominale\t        N–1\t                Contraste avec moyenne des modalités précédentes\n",
    "# Backward Difference\t    Ordinale\t        N–1\t                Contraste avec moyenne des modalités suivantes\n",
    "# Binary Encoding\t        Nominale\t        log₂(N)\t            Encodage binaire de l’index\n",
    "# Hashing Encoding\t        Nominale\t        n_components\t    Hash des modalités sur colonnes fixes\n",
    "# Label Encoding\t        Ordinale\t        1\t                Entier arbitraire\n",
    "# Ordinal Encoding\t        Ordinale\t        1\t                Rang croissant des modalités\n",
    "# Target Encoding\t        Nominale/Ordinale\t1\t                Moyenne de la cible par modalité\n",
    "# Mean Encoding\t            Nominale/Ordinale\t1\t                Idem Target Encoding\n",
    "# Frequency Encoding\t    Nominale/Ordinale\t1\t                Fréquence d'apparition\n",
    "# Leave-One-Out\t            Nominale/Ordinale\t1\t                Moyenne de la cible, sauf ligne courante\n",
    "# James-Stein Encoding\t    Nominale/Ordinale\t1\t                Moyenne pondérée par variance intercatégorie\n",
    "# M-Estimate Encoding\t    Nominale/Ordinale\t1\t                Moyenne cible lissée vers moyenne globale\n",
    "# Probability Ratio\t        Ordinale, binaire\t1\t                Log du ratio de probas classe 1 / classe 0\n",
    "# WOE Encoding\t            Ordinale, binaire\t1\t                Log( %positif / %négatif )\n",
    "# Thermometer Encoding\t    Ordinale\t        N\t                1 si la modalité est ≤ à une valeur\n",
    "# Trigonométrique (sin/cos)\tCyclique\t        2\t                Encode la cyclicité\n",
    "# Fourier / Radial\t        Cyclique\t        Variable\t        Approximation périodique (base)\n",
    "ohe_enc_col = ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "ohe_enc = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "# Appliquer OneHotEncoder\n",
    "X_train_enc_cat = ohe_enc.fit_transform(X_train[ohe_enc_col])\n",
    "X_test_enc_cat = ohe_enc.transform(X_test[ohe_enc_col])\n",
    "# Ajout des colonnes encodées à un DataFrame car le resultat de enc.fit/transform estun ndarray sans index/colonnes\n",
    "X_train_cat_df = pd.DataFrame(X_train_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_train.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_test.index)\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train = pd.concat([X_train.drop(columns=ohe_enc_col), X_train_cat_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=ohe_enc_col), X_test_cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66868f91",
   "metadata": {},
   "source": [
    "# 6. Selection and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a84d51",
   "metadata": {},
   "source": [
    "### 6.1 Quick Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression configuration\n",
    "logit_config = {\n",
    "    \"target\": \"station_en_fonctionnement\",\n",
    "    \"features\": ['nombre_bornettes_libres', 'nombre_total_velos_disponibles']\n",
    "}\n",
    "\n",
    "# Coefficient adjustment configuration\n",
    "adjustment_config = {\n",
    "    \"nombre_bornettes_libres\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 68)\n",
    "    },\n",
    "    \"nombre_total_velos_disponibles\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 65)\n",
    "    },\n",
    "    # \"nombre_total_velos_disponibles\": {\n",
    "    #     \"type\": \"inverse\"\n",
    "    # },\n",
    "}\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "subset = logit_config[\"features\"]+[logit_config[\"target\"]]\n",
    "cls.logit_analysis(df_train[subset], logit_config, adjustment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad389e",
   "metadata": {},
   "source": [
    "### 6.2 SMOTE/Under/Over Sampling (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002342df",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16adc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7d3ec",
   "metadata": {},
   "source": [
    "### 6.3 Adding threashold (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f3f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af0f26f",
   "metadata": {},
   "source": [
    "### 6.4 Switching to other models (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d759db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b063c57",
   "metadata": {},
   "source": [
    "# 7. Best model application and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling sur la base d'entraînement\n",
    "smote = SMOTE()\n",
    "X_train_b_smote, y_train_b_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Définition et entraînement du modèle\n",
    "clf_XGB_opti = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_opti.fit(X_train_b_smote, y_train_b_smote)\n",
    "\n",
    "# Prédiction du modèle (Seuil précédemment établi)\n",
    "preds = np.where(clf_XGB_opti.predict_proba(X_test)[:, 1] > 0.026, 1, 0)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Modèle avec rééchantillonnage et optimisation du seuil\")\n",
    "print(\"F1-Score : \", f1_score(preds, y_test))\n",
    "print(confusion_matrix(preds, y_test))\n",
    "\n",
    "# Modèle de base\n",
    "print(\"Modèle de base\")\n",
    "clf_XGB_base = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_base.fit(X_train, y_train)\n",
    "print(\"F1-Score : \", f1_score(clf_XGB_base.predict(X_test), y_test))\n",
    "print(confusion_matrix(clf_XGB_base.predict(X_test), y_test))\n",
    "plot_importance(clf_XGB_base) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
