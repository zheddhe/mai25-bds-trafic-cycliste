{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# transformation\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# metrics and evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from scipy.stats import chi2_contingency, probplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# # graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# # for jupyter notebook display management\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 2.1.1 VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### 2.1.2 VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib_raw = dfc.load_dataset_from_config('velib_dispo_data', sep=';')\n",
    "\n",
    "if df_disp_velib_raw is not None and isinstance(df_disp_velib_raw, pd.DataFrame):\n",
    "    display(df_disp_velib_raw.head())\n",
    "    dfc.log_general_info(df_disp_velib_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_disp_velib_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_disp_velib_raw))\n",
    "    df_disp_velib = dfc.normalize_column_names(df_disp_velib_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_disp_velib.info()\n",
    "display(df_disp_velib.head())\n",
    "df_cpt_velo_desc = df_disp_velib.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_disp_velib.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)\n",
    "dfc.display_variable_info(df_disp_velib, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Cross Distribution inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la distribution d'une variable spécique en relation avec les autres de son dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "dfc.analyze_by_reference_variable(df_disp_velib, ref_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée de la distribution d'une variable spécifique en fonction d'autres variables (quantitatives ou qualitatives) du dataframe\n",
    "ref_col = 'station_en_fonctionnement' # ici notre variable cible\n",
    "cross_columns = [ref_col] + ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "dfc.log_cross_distributions(\n",
    "    df_disp_velib[cross_columns], \n",
    "    ref_col\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée d'une variable en fonction d'une autre\n",
    "ref_col = 'borne_de_paiement_disponible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "display(pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse croisée des d'une variable en fonction d'une autre (en conservant les NaN)\n",
    "ref_col = 'station_opening_hours'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "ref_cross_tab = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab)\n",
    "\n",
    "# Catégorisation et normalisation\n",
    "ref_col_val_norm = np.where(\n",
    "    df_disp_velib[ref_col].isin(ref_cross_tab[ref_cross_tab['OUI'] >= 0.8].index.tolist()), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "df_disp_velib[ref_col] = ref_col_val_norm\n",
    "ref_cross_tab_norm = pd.crosstab(df_disp_velib[ref_col], df_disp_velib[target_col], dropna=False, normalize=True)\n",
    "display(ref_cross_tab_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Signifiance against target evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification de la signifiance des variables explicatives par rapport à une variable cible\n",
    "ref_col = 'retour_velib_possible'\n",
    "target_col = 'station_en_fonctionnement'\n",
    "# Génération des colonnes dummies pour ref_col\n",
    "ref_col_dummies = pd.get_dummies(df_disp_velib[ref_col], prefix=ref_col)\n",
    "print(\"Colonnes dummies générées :\", list(ref_col_dummies.columns))\n",
    "# Pour chaque modalité de cette variable (dummy 0/1), tester sa signifiance avec la variable cible\n",
    "for col in ref_col_dummies:\n",
    "    # Test du Chi-Deux\n",
    "    cross_tab = pd.crosstab(df_disp_velib[target_col], ref_col_dummies[col])\n",
    "    if cross_tab.shape[1] != 2:\n",
    "        print(f\"⚠️ Modalité [{col}] ignorée (1 seule valeur présente)\")\n",
    "        continue\n",
    "    stat, p, _, _ = chi2_contingency(cross_tab)\n",
    "    # V de Cramer\n",
    "    V_Cramer = np.sqrt(\n",
    "        stat/cross_tab.values.sum())\n",
    "    # On affiche uniquement les variables significatives et dont le V de Cramer est supérieur à 0.1\n",
    "    # Faible : Valeur autour de 0.1 ;\n",
    "    # Moyenne : Valeur autour de 0.3 ;\n",
    "    # Elevée : Valeur autour et supérieure à 0.5.\n",
    "    # Lorsque la valeur du V de Cramer est très élevée (aux alentours de 0.8 et plus), on soupçonne généralement de la multicolinéarité.\n",
    "    result = 'significative' if (p < 0.05) and (V_Cramer > 0.1) else 'NON signficative'  # type: ignore\n",
    "    print(f\"Variable [{col}] {result} Vs [{target_col}]: p-value[{p:.5f}], V_Cramer[{V_Cramer:.5f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 2.2.2 VELIB DISPO (Optional Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and duplicates management\n",
    "df_disp_velib_orig = df_disp_velib.copy()\n",
    "df_disp_velib = df_disp_velib.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation manuelle de la variable cible \n",
    "# /!\\ attention /!\\ les transformation d'encodage meme les plus simples ne sont pas sans fuite de données car elles changent la structure\n",
    "# exemple : un get_dummies qui va créer une colonne sur une valeur rare influence par les colonnes additionnelles (creation de sub features)\n",
    "df_disp_velib.station_en_fonctionnement = df_disp_velib.station_en_fonctionnement.apply(lambda x: 1 if x=='OUI' else 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "for col in df_disp_velib.select_dtypes(include='number').columns:\n",
    "    probplot(df_disp_velib[col], dist=\"norm\", plot=plt)\n",
    "    plt.suptitle(f\"Column {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "# 4. Division in Train/Test\n",
    "Lorsqu'il s'agit de données ***chronologiques*** où la tâche consiste à faire des prévisions, les ensembles d'**entraînement**, de **validation** et de **test** doivent être sélectionnés en séparant les données le long de l'axe temporel. C'est-à-dire que les données les plus anciennes sont utilisées pour l'entraînement, les plus récentes pour la validation et les dernières chronologiquement pour les tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temporaire) Ajustement : enlever les variables non retravaillées pour le moment\n",
    "df_disp_velib = df_disp_velib.drop(columns=['identifiant_station',\n",
    "                                            'nom_station', \n",
    "                                            'actualisation_de_la_donnee', \n",
    "                                            'coordonnees_geographiques', \n",
    "                                            'nom_communes_equipees',\n",
    "                                            'code_insee_communes_equipees',\n",
    "                                            'station_opening_hours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features (X) et target (y) pour train et test\n",
    "target_col = 'station_en_fonctionnement'\n",
    "features = df_disp_velib.drop(target_col, axis=1)\n",
    "target = df_disp_velib[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=66)\n",
    "print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set (X/y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "Règle d'or : Toute opération qui \"***apprend***\" des données (i.e. utilise l’ensemble des valeurs pour calculer quelque chose) doit être faite après le split train/test — c’est-à-dire uniquement sur le train.\n",
    "\n",
    "| Type de transformation                                                                                    | À faire avant le split ?                    | Détails                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ Création de features basées sur les colonnes existantes (ex: `BMI = weight / height²`)                  | **Avant**                                   | Pas de risque de fuite car c’est purement déterministe.            |\n",
    "| ⚠️ Calculs dépendant de la distribution (moyennes, encodage fréquentiel, imputation par la médiane, etc.) | **Après** (sur le train uniquement)         | Risque de fuite de données si appliqué sur l’ensemble avant split. |\n",
    "| ✅ Ajout de features exogènes fixes (données météo, géographiques, calendaires, etc.)                      | **Avant**                                   | Pas de dépendance au `target` ni à la répartition train/test.      |\n",
    "| ⚠️ Encoding (`LabelEncoder`, `OneHot`, `TargetEncoding`, etc.)                                            | **Fit sur train, transform sur train/test** | Toujours fitter uniquement sur le `train`.                         |\n",
    "| ⚠️ Standardisation / normalisation (Scaler)                                                               | **Fit sur train, transform sur train/test** | Pareil : `.fit()` sur train, `.transform()` sur test.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 5.1 Modification localisées sur les variables d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple (fictif) de modification localisée des données de test en fonction de la proximité à la médiane d'autre variables\n",
    "# mask = (\n",
    "#     (X_train['Gender'].isna()) &\n",
    "#     (abs(X_train['Age'] - 30) > abs(X_train['Age'] - 41)) & # L’âge est plus proche de 41 que de 30\n",
    "#     (X_train['Previously_Insured'] == 0) & # La personne n’était pas assurée auparavant\n",
    "#     (X_train['Vehicle_Damage'] == 1) # Elle a subi un dommage sur son véhicule\n",
    "# )\n",
    "# X_train.loc[mask, 'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple (fictif) de modification des données de test par répartition spécifique entre deux valeurs 0 et 1 sur 100 (dans une liste)\n",
    "# proportion_tab = [0] * 55 + [1] * 45\n",
    "# mask = (\n",
    "#     (X_train['Gender'].isna())\n",
    "# )\n",
    "# X_train.loc[mask, 'Gender'] = X_train.loc[mask, 'Gender'].apply(lambda x: random.choice(proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### 5.2.1 Scaling (données quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ni outlier ni distribution loi normale : min/max\n",
    "# - sans outlier mais distribution loi normale : standard\n",
    "# - avec outlier : Robust \n",
    "mm_scal = MinMaxScaler()\n",
    "r_scal = RobustScaler()\n",
    "s_scal = StandardScaler()\n",
    "\n",
    "r_scal_col = ['velos_mecaniques_disponibles', 'nombre_total_velos_disponibles']\n",
    "df_disp_velib[r_scal_col] = s_scal.fit_transform(df_disp_velib[r_scal_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoding (données qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique                 Type                Colonnes créées     Principe\n",
    "# get_dummies()\t            Nominale\t        N (ou N–1)\t        Binaire par modalité\n",
    "# OneHotEncoder\t            Nominale, Cyclique\tN\t                Colonne 0/1 par modalité\n",
    "# Sum Encoding\t            Nominale\t        N–1\t                Différence avec moyenne globale\n",
    "# Helmert Encoding\t        Nominale\t        N–1\t                Contraste avec moyenne des modalités précédentes\n",
    "# Backward Difference\t    Ordinale\t        N–1\t                Contraste avec moyenne des modalités suivantes\n",
    "# Binary Encoding\t        Nominale\t        log₂(N)\t            Encodage binaire de l’index\n",
    "# Hashing Encoding\t        Nominale\t        n_components\t    Hash des modalités sur colonnes fixes\n",
    "# Label Encoding\t        Ordinale\t        1\t                Entier arbitraire\n",
    "# Ordinal Encoding\t        Ordinale\t        1\t                Rang croissant des modalités\n",
    "# Target Encoding\t        Nominale/Ordinale\t1\t                Moyenne de la cible par modalité\n",
    "# Mean Encoding\t            Nominale/Ordinale\t1\t                Idem Target Encoding\n",
    "# Frequency Encoding\t    Nominale/Ordinale\t1\t                Fréquence d'apparition\n",
    "# Leave-One-Out\t            Nominale/Ordinale\t1\t                Moyenne de la cible, sauf ligne courante\n",
    "# James-Stein Encoding\t    Nominale/Ordinale\t1\t                Moyenne pondérée par variance intercatégorie\n",
    "# M-Estimate Encoding\t    Nominale/Ordinale\t1\t                Moyenne cible lissée vers moyenne globale\n",
    "# Probability Ratio\t        Ordinale, binaire\t1\t                Log du ratio de probas classe 1 / classe 0\n",
    "# WOE Encoding\t            Ordinale, binaire\t1\t                Log( %positif / %négatif )\n",
    "# Thermometer Encoding\t    Ordinale\t        N\t                1 si la modalité est ≤ à une valeur\n",
    "# Trigonométrique (sin/cos)\tCyclique\t        2\t                Encode la cyclicité\n",
    "# Fourier / Radial\t        Cyclique\t        Variable\t        Approximation périodique (base)\n",
    "ohe_enc_col = ['borne_de_paiement_disponible', 'retour_velib_possible']\n",
    "ohe_enc = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "# Appliquer OneHotEncoder\n",
    "X_train_enc_cat = ohe_enc.fit_transform(X_train[ohe_enc_col])\n",
    "X_test_enc_cat = ohe_enc.transform(X_test[ohe_enc_col])\n",
    "# Ajout des colonnes encodées à un DataFrame car le resultat de enc.fit/transform estun ndarray sans index/colonnes\n",
    "X_train_cat_df = pd.DataFrame(X_train_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_train.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_test.index)  # type: ignore\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train = pd.concat([X_train.drop(columns=ohe_enc_col), X_train_cat_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=ohe_enc_col), X_test_cat_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "# 6. Selection and Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "## 6.0 Standard Canvas\n",
    "\n",
    "✅ Résumé des meilleurs préfixes\n",
    "| Rôle                    | Préfixe recommandé     | Exemple                       |\n",
    "|-------------------------|------------------------|-------------------------------|\n",
    "| Classifier              | `clf_`                 | `clf_rf`, `clf_svc`           |\n",
    "| Regressor               | `reg_`                 | `reg_ridge`                   |\n",
    "| Estimator (générique)   | `est_`                 | `est_model`                   |\n",
    "| Meta-estimator / search | `search_`, `meta_`     | `search_gs`, `meta_cv_model` |\n",
    "| Cross-val splitter      | `cv_`                  | `cv_kfold`, `cv_stratified`  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir les estimators unitaires (classifier/regressor)\n",
    "clf_svc = SVC()\n",
    "clf_rfc = RandomForestClassifier()\n",
    "# Definir les dictionnaires d'hyperparamètres des meta-estimators\n",
    "#Gridsearch\n",
    "param_gs = {\n",
    "    'C': [0.1, 1, 10], \n",
    "    'kernel': ['linear', 'rbf'],\n",
    "}\n",
    "#Randomsearch\n",
    "param_rs = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "}\n",
    "#BayesSearch (défini automatiquement une répartition optimisée dans le range de valeur)\n",
    "param_bs = {\n",
    "   'n_estimators': (100, 300),\n",
    "   'max_depth': (10, 50),\n",
    "}\n",
    "# Définir les meta-estimators\n",
    "meta_gscv = GridSearchCV(estimator=clf_svc, param_grid=param_gs, cv=5, scoring='accuracy')\n",
    "meta_rscv = RandomizedSearchCV(estimator=clf_rfc, param_distributions=param_rs, n_iter=9, cv=5, scoring='accuracy', random_state=42)\n",
    "meta_bscv = BayesSearchCV(estimator=clf_rfc, search_spaces=param_bs, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "# Recherche et affichage des hyperparamètre optimisés\n",
    "meta_gscv.fit(X_train, y_train)\n",
    "meta_rscv.fit(X_train, y_train)\n",
    "meta_bscv.fit(X_train, y_train)\n",
    "print(meta_gscv.best_params_)\n",
    "print(meta_rscv.best_params_)\n",
    "print(meta_bscv.best_params_) # type: ignore[attr-defined]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## 6.1 Quick Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression configuration\n",
    "logit_config = {\n",
    "    \"target\": \"station_en_fonctionnement\",\n",
    "    \"features\": ['nombre_bornettes_libres', 'nombre_total_velos_disponibles']\n",
    "}\n",
    "\n",
    "# Coefficient adjustment configuration\n",
    "adjustment_config = {\n",
    "    \"nombre_bornettes_libres\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 68)\n",
    "    },\n",
    "    \"nombre_total_velos_disponibles\": {\n",
    "        \"type\": \"normalize\",\n",
    "        \"range\": (0, 65)\n",
    "    },\n",
    "    # \"nombre_total_velos_disponibles\": {\n",
    "    #     \"type\": \"inverse\"\n",
    "    # },\n",
    "}\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "subset = logit_config[\"features\"]+[logit_config[\"target\"]]\n",
    "cls.logit_analysis(df_train[subset], logit_config, adjustment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## 6.2 SMOTE/Under/Over Sampling (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 6.3 Adding threashold (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, LogisticRegression())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "## 6.4 Switching to other models (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.cross_validation_with_resampling_and_threshold(X_train, y_train, XGBClassifier(eval_metric=\"error\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "# 7. Best model application and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling sur la base d'entraînement\n",
    "smote = SMOTE()\n",
    "X_train_b_smote, y_train_b_smote = smote.fit_resample(X_train, y_train)  # type: ignore\n",
    "\n",
    "# Définition et entraînement du modèle\n",
    "clf_XGB_opti = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_opti.fit(X_train_b_smote, y_train_b_smote)\n",
    "\n",
    "# Prédiction du modèle (Seuil précédemment établi)\n",
    "preds = np.where(clf_XGB_opti.predict_proba(X_test)[:, 1] > 0.026, 1, 0)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Modèle avec rééchantillonnage et optimisation du seuil\")\n",
    "print(\"F1-Score : \", f1_score(preds, y_test))\n",
    "print(confusion_matrix(preds, y_test))\n",
    "\n",
    "# Modèle de base\n",
    "print(\"Modèle de base\")\n",
    "clf_XGB_base = XGBClassifier(eval_metric=\"error\")\n",
    "clf_XGB_base.fit(X_train, y_train)\n",
    "print(\"F1-Score : \", f1_score(clf_XGB_base.predict(X_test), y_test))\n",
    "print(confusion_matrix(clf_XGB_base.predict(X_test), y_test))\n",
    "plot_importance(clf_XGB_base) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
