{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# metrics and evaluation\n",
    "from scipy.stats import probplot, anderson, chi2_contingency, pearsonr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "# for jupyter notebook display management\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "\n",
    "# graphical missingno\n",
    "import missingno as msno\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc\n",
    "import smartcheck.dataframe_project_specific as dfps\n",
    "import smartcheck.paths as pth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo_raw = dfc.load_dataset_from_config('velo_comptage_data', sep=';')\n",
    "\n",
    "if df_cpt_velo_raw is not None and isinstance(df_cpt_velo_raw, pd.DataFrame):\n",
    "    df_cpt_velo = dfc.normalize_column_names(df_cpt_velo_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">On a normalisé les noms de colonnes du data set afin de faciliter leur manipulation dans le code. La norme utilisée est de type **snake_case**\n",
    ">  - Remplacement des caractères spéciaux et espaces par des underscores\n",
    ">  - Conversion en minuscules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Sauvegarde en mémoire de l'origine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup before missing value management\n",
    "df_cpt_velo_bckp_orig = df_cpt_velo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df_cpt_velo = df_cpt_velo_bckp_orig.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "#### Search for general informations, duplicates and missing values stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cpt_velo.head())\n",
    "dfc.log_general_info(df_cpt_velo)\n",
    "nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_cpt_velo)\n",
    "if nb_first != nb_total:\n",
    "    print(dfc.duplicates_index_map(df_cpt_velo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">Le dataset ***comptage velo*** présente **942554** observations (lignes) avec **16** variables (colonnes)\n",
    ">  - Il ne possède **aucune** observation en doublon et toutes les observations possèdent **au moins** une information.\n",
    ">  - La proportion d'informations manquantes (détail par variable ci-dessous) est de **3.38%**\n",
    ">  - Pour notre objectif (identification des zones de trafic dense en vue d'aménagements), aucune variable ne semble donner une notion d'affluence par zone et horaire, donc nous serions face à une problématique d'analyse exploratoire par clusterisation sans supervision, si la problématique avait été de prédire le traffic alors la variable comptage_horaire aurait été une bonne variable cible (et le problème de type régression supervisée)\n",
    ">\n",
    ">| Variable | % informations manquantes |\n",
    ">|-|-|\n",
    ">| identifiant_du_compteur | 3.98% |\n",
    ">| identifiant_du_site_de_comptage | 3.98% |\n",
    ">| nom_du_site_de_comptage | 3.98% |\n",
    ">| date_d_installation_du_site_de_comptage | 3.98% |\n",
    ">| lien_vers_photo_du_site_de_comptage | 5.02% |\n",
    ">| coordonnees_geographiques | 3.98% |\n",
    ">| identifiant_technique_compteur | 5.02% |\n",
    ">| id_photos | 5.02% |\n",
    ">| test_lien_vers_photos_du_site_de_comptage | 5.02% |\n",
    ">| id_photo_1 | 5.02% |\n",
    ">| url_sites | 3.98% |\n",
    ">| type_dimage | 5.02% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation des valeur NA graphiquement\n",
    "msno.matrix(df_cpt_velo_bckp_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">Le graphique de répartition des valeurs manquantes montre que les observations manquantes sont regroupées sur des plages d'index contiguës ce qui facilite l'identification des causes en regroupant sur les absences de valeur d'une des colonnes suivantes:\n",
    "> - lien_vers_photo_du_site_de_comptage\n",
    "> - identifiant_technique_compteur\n",
    "> - id_photos\n",
    "> - test_lien_vers_photos_du_site_de_comptage\n",
    "> - id_photo_1\n",
    "> - type_dimage\n",
    ">\n",
    ">On observe que l'absence de valeur concerne un cluster de toutes les colonnes ci-dessus et également plusieurs autre clusters avec les colonnes additionnelles suivantes:\n",
    "> - identifiant_du_compteur\n",
    "> - identifiant_du_site_de_comptage\n",
    "> - nom_du_site_de_comptage\n",
    "> - date_d_installation_du_site_de_comptage\n",
    "> - coordonnees_geographiques\n",
    "> - url_sites\n",
    ">\n",
    ">*[insérer **graphique à jour** cellule ci-dessus]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Missing value correlation exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_compteur_na = df_cpt_velo[df_cpt_velo.lien_vers_photo_du_site_de_comptage.isna()].groupby('nom_du_compteur').count()\n",
    "display(liste_compteur_na, liste_compteur_na.index.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">On constate que seuls quelques noms de compteur concentrent l'ensemble des observations manquantes (ci-après) et que pour ces noms de compteur, les informations absentes le sont uniformément sur l'ensemble des variables absentes\n",
    ">\n",
    ">Les noms de compteurs identifiés sont en fait des valeurs erronées transitoires et leur nom de compteur valide peut être inféré et recoupé avec la base [Comptage vélo - Compteurs](https://parisdata.opendatasoft.com/explore/dataset/comptage-velo-compteurs/table/?disjunctive.counter&disjunctive.name&disjunctive.nom_compteur&disjunctive.id&disjunctive.id_compteur)\n",
    ">\n",
    "> - '10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike IN]'\n",
    "> - '10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike OUT]'\n",
    "> - '27 quai de la Tournelle 27 quai de la Tournelle Vélos NO-SE'\n",
    "> - '27 quai de la Tournelle 27 quai de la Tournelle Vélos SE-NO'\n",
    "> - '35 boulevard de Ménilmontant NO-SE' (seul site du lot possédant en plus les données) :\n",
    ">   - url_sites\n",
    ">   - coordonnees_geographiques\n",
    ">   - date_d_installation_du_site_de_comptage\n",
    ">   - nom_du_site_de_comptage\n",
    ">   - identifiant_du_site_de_comptage\n",
    ">   - identifiant_du_compteur\n",
    "> - 'Face au 48 quai de la marne Face au 48 quai de la marne Vélos NE-SO'\n",
    "> - 'Face au 48 quai de la marne Face au 48 quai de la marne Vélos SO-NE'\n",
    "> - 'Pont des Invalides N-S'\n",
    "> - 'Quai des Tuileries Quai des Tuileries Vélos NO-SE'\n",
    "> - 'Quai des Tuileries Quai des Tuileries Vélos SE-NO'\n",
    "> - 'Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos E-O'\n",
    "> - 'Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos O-E'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"avenue de la Grande Armée\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"avenue de la Grande Armée\")\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"avenue de la Grande Armée\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"avenue de la grande armée\" montre qu'ils sont une version préliminaire de \"10 avenue de la Grande Armée SE-NO\" (à partir de mars 2025 ils sont regroupés sur 10 avenue de la Grande Armée SE-NO et n'émettent plus d'information)\n",
    "> - \"10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike IN]\" à propager vers \"10 avenue de la Grande Armée SE-NO\"\n",
    "> - \"10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike OUT]\" à propager vers \"10 avenue de la Grande Armée SE-NO\"\n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement les compteurs ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"27 quai de la Tournelle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"27 quai de la Tournelle\")\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"27 quai de la Tournelle\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"quai de la Tournelle\" montre un simple coquille sur période de 4 mois des compteurs officiels (entre novembre 2024 et février 2025)\n",
    "> - \"27 quai de la Tournelle 27 quai de la Tournelle Vélos NO-SE\" à propager vers \"27 quai de la Tournelle NO-SE\"\n",
    "> - \"27 quai de la Tournelle 27 quai de la Tournelle Vélos SE-NO\" à propager vers \"27 quai de la Tournelle SE-NO\" \n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement les compteurs ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"Face au 48 quai de la marne\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Face au 48 quai de la marne\")\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Face au 48 quai de la marne\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"Face au 48 quai de la marne\" montre un simple coquille sur période de 4 mois des compteurs officiels (entre novembre 2024 et février 2025)\n",
    "> - \"Face au 48 quai de la marne Face au 48 quai de la marne Vélos NE-SO\" à propager vers \"Face au 48 quai de la marne NE-SO\"\n",
    "> - \"Face au 48 quai de la marne Face au 48 quai de la marne Vélos SO-NE\" à propager vers \"Face au 48 quai de la marne SO-NE\"\n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement les compteurs ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"Pont des Invalides\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Pont des Invalides\")\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Pont des Invalides\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"Pont des Invalides\" montre un simple changement de nom d'un des compteurs (entre mars 2024 et février 2025)\n",
    "> - \"Pont des Invalides N-S\" à propager vers \"Pont des Invalides (couloir bus) N-S\"\n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement le compteur ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"Quai des Tuileries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Quai des Tuileries\")\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Quai des Tuileries\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"Quai des Tuileries\" montre un simple coquille sur période de 4 mois des compteurs officiels (entre novembre 2024 et février 2025)\n",
    "> - \"Quai des Tuileries Quai des Tuileries Vélos NO-SE\" à propager vers \"Quai des Tuileries NO-SE\"\n",
    "> - \"Quai des Tuileries Quai des Tuileries Vélos SE-NO\" à propager vers \"Quai des Tuileries SE-NO\"\n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement les compteurs ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"Totem 64 Rue de Rivoli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Totem 64 Rue de Rivoli\")\n",
    "\n",
    "col_count = ['nom_du_compteur', 'coordonnees_geographiques']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())\n",
    "\n",
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"Totem 64 Rue de Rivoli\")\n",
    "col_count = ['nom_du_compteur', 'mois_annee_comptage']\n",
    "display(df_cpt_velo[mask].groupby(col_count)[col_count].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"Totem 64 Rue de Rivoli\" montre un simple coquille sur période d'un an des compteurs officiels (entre mars 2024 et février 2025)\n",
    "> - \"Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos E-O\" à propager vers \"Totem 64 Rue de Rivoli E-O\"\n",
    "> - \"Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos O-E\" à propager vers \"Totem 64 Rue de Rivoli O-E\" \n",
    ">\n",
    ">Nous pouvons donc propager les bonnes informations manquante depuis la source et renommer correctement les compteurs ci-dessus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "#### Missing value correlation for \"boulevard de Ménilmontant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.nom_du_compteur.str.contains(\"boulevard de Ménilmontant\")\n",
    "col_count = [\n",
    "    'nom_du_compteur', \n",
    "    'lien_vers_photo_du_site_de_comptage',\n",
    "    'identifiant_technique_compteur',\n",
    "    'id_photos',\n",
    "    'test_lien_vers_photos_du_site_de_comptage',\n",
    "    'id_photo_1',\n",
    "    'type_dimage',\n",
    "]\n",
    "display(df_cpt_velo[mask].groupby(col_count, dropna=False)[col_count].count())\n",
    "\n",
    "agg_counts = (\n",
    "    df_cpt_velo\n",
    "    .groupby(col_count, dropna=False)\n",
    "    .size()  # équivalent à count(), mais plus direct\n",
    "    .reset_index(name='nb_occurrences')\n",
    "    .groupby('nom_du_compteur')['nb_occurrences']\n",
    "    .count()\n",
    "    .reset_index(name='nb_occurrences')\n",
    ")\n",
    "print(f\"Nombre d'agrégats uniques sur les colonnes {col_count} regroupés par 'nom_du_compteur'\")\n",
    "display(agg_counts.head())\n",
    "print(\"En filtrant sur les regroupements de ces aggrégats supérieurs strictement à 1\"\n",
    "      \" (identification des groupes de colonnes avec plusieurs combinaisons de valeurs)\")\n",
    "display(agg_counts.loc[agg_counts['nb_occurrences'] > 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">L'analyse pour les compteurs avec infos manquantes \"boulevard de Ménilmontant\" montre que seules les données spécifiques suivantes sont manquantes de manière uniforme\n",
    "> - 'lien_vers_photo_du_site_de_comptage'\n",
    "> - 'identifiant_technique_compteur'\n",
    "> - 'id_photos'\n",
    "> - 'test_lien_vers_photos_du_site_de_comptage'\n",
    "> - 'id_photo_1'\n",
    "> - 'type_dimage'\n",
    ">\n",
    ">Nous décidons de ne pas faire d'action sur ces colonnes car non pertinentes (nous ne disposons par ailleurs pas de la source sur [Comptage vélo - Compteurs](https://parisdata.opendatasoft.com/explore/dataset/comptage-velo-compteurs/table/?disjunctive.counter&disjunctive.name&disjunctive.nom_compteur&disjunctive.id&disjunctive.id_compteur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Analysis of 'identifiant_du_compteur', 'identifiant_du_site_de_comptage', 'nom_du_compteur', 'nom_du_site_de_comptage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_identifiants = df_cpt_velo[['identifiant_du_compteur', 'identifiant_du_site_de_comptage', 'nom_du_compteur', 'nom_du_site_de_comptage', 'comptage_horaire']]\n",
    "df_cpt_grouped = df_identifiants.groupby(['identifiant_du_compteur', 'nom_du_compteur']).comptage_horaire.count().reset_index()\n",
    "df_site_grouped = df_identifiants.groupby(['identifiant_du_site_de_comptage', 'nom_du_site_de_comptage']).identifiant_du_compteur.count().reset_index()\n",
    "print(\"Nombre de doublon sur les couples identifiant_du_compteur/nom_du_compteur:\",df_cpt_grouped.duplicated().sum())\n",
    "display(df_cpt_grouped)\n",
    "print(\"Nombre de doublon sur les couples identifiant_du_site_de_comptage/nom_du_site_de_comptage:\", df_site_grouped.duplicated().sum())\n",
    "display(df_site_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.display_variable_info(df_cpt_velo.identifiant_du_compteur)\n",
    "dfc.display_variable_info(df_cpt_velo.nom_du_compteur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.display_variable_info(df_cpt_velo.identifiant_du_site_de_comptage)\n",
    "dfc.display_variable_info(df_cpt_velo.nom_du_site_de_comptage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_cpt_velo.apply(lambda row: \n",
    "                  True if pd.isna(row.nom_du_site_de_comptage) else \n",
    "                  True if row.nom_du_site_de_comptage in row.nom_du_compteur else \n",
    "                  False, axis=1)\n",
    "display(\"lignes dont le nom du site n'est pas complètement inclus dans le nom du compteur\",df_cpt_velo[~mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_diff = partial(\n",
    "    dfc.extract_difference,\n",
    "    source_col=\"nom_du_site_de_comptage\",\n",
    "    target_col=\"nom_du_compteur\",\n",
    "    # nan_placeholder=\"__NaN__\",\n",
    "    # not_found_placeholder=\"__NOT_FOUND__\"\n",
    ")\n",
    "\n",
    "# Ajout d'une nouvelle colonne orientation compteur extraite de la fin du nom du compteur\n",
    "df_cpt_velo[\"orientation_compteur\"] = df_cpt_velo.apply(\n",
    "    custom_diff,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Analysis of 'orientation_compteur'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.display_variable_info(df_cpt_velo.orientation_compteur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">On peut confirmer que les variables identifiant_du_compteur/nom_du_compteur et identifiant_du_site_de_comptage/nom_du_site_de_comptage ont une relation 1-1. Il faudra donc traiter ces informations redondantes et décider laquelle conserver\n",
    ">\n",
    ">Les variables liées aux \"noms\" (compteur ou site de comptage), contiennent plus d'information exploitables que leur contrepartie identifiant numérique notamment des information géographiques et pour le nom du compteur une information additionnelle d'orientation dont voici les valeurs extraites sur tout le data set (avec leur répartitions normalisée)\n",
    ">\n",
    ">|orientation_compteur|normalized count|\n",
    ">|-|-|\n",
    ">| NE-SO        |  0.164345|\n",
    ">| SO-NE        |  0.153553|\n",
    ">| NO-SE        |  0.150981|\n",
    ">| SE-NO        |  0.130895|\n",
    ">| N-S          |  0.097736|\n",
    ">| E-O          |  0.095101|\n",
    ">| S-N          |  0.093293|\n",
    ">| O-E          |  0.074274|\n",
    ">|\\[nom_du_site_de_comptage\\] EMPTY  |  0.039822|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### Analysis of 'type_image', 'url_sites' , 'id_photo_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc.display_variable_info(df_cpt_velo.type_dimage)\n",
    "dfc.display_variable_info(df_cpt_velo.id_photo_1)\n",
    "dfc.display_variable_info(df_cpt_velo.url_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### Global description and correlation for quantitative variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo_desc = df_cpt_velo.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_desc = df_cpt_velo.select_dtypes(include='object').describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_cpt_velo.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">Description des variables quantitatives brutes\n",
    ">\n",
    ">|Stat   |  comptage_horaire   |    identifiant_du_site_de_comptage   |        \n",
    ">|-|-|-|\n",
    ">|count\t| 942554.000000      |    9.050200e+05\t                    |\n",
    ">|mean\t| 77.879018          |    1.348171e+08\t                    |\n",
    ">|std    |  106.928569         |    7.579019e+07\t                    |\n",
    ">|min    |  0.000000           |    1.000031e+08\t                    |\n",
    ">|25%    |  11.000000          |    1.000475e+08\t                    |\n",
    ">|50%    |  42.000000          |    1.000560e+08\t                    |\n",
    ">|75%    |  97.000000          |    1.000563e+08\t                    |\n",
    ">|max    |  3070.000000        |    3.000303e+08\t                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Traitement des valeurs manquantes par propagation des infos des nom de compteur identifié (source->cible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colonnes à copier globalement pour tous les clusters de nom de compteur depuis une source peuplée\n",
    "colonnes_a_copier = [\n",
    "    'nom_du_compteur',\n",
    "    'identifiant_du_compteur',\n",
    "    'identifiant_du_site_de_comptage', \n",
    "    'nom_du_site_de_comptage',\n",
    "    'date_d_installation_du_site_de_comptage',\n",
    "    'coordonnees_geographiques',\n",
    "    'url_sites',\n",
    "]\n",
    "dico_repl = {\n",
    "    'cibles':[\n",
    "        '10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike IN]',\n",
    "        '10 avenue de la Grande Armée 10 avenue de la Grande Armée [Bike OUT]',\n",
    "        '27 quai de la Tournelle 27 quai de la Tournelle Vélos NO-SE',\n",
    "        '27 quai de la Tournelle 27 quai de la Tournelle Vélos SE-NO',\n",
    "        \"Face au 48 quai de la marne Face au 48 quai de la marne Vélos NE-SO\",\n",
    "        \"Face au 48 quai de la marne Face au 48 quai de la marne Vélos SO-NE\",\n",
    "        \"Pont des Invalides N-S\",\n",
    "        \"Quai des Tuileries Quai des Tuileries Vélos NO-SE\",\n",
    "        \"Quai des Tuileries Quai des Tuileries Vélos SE-NO\",\n",
    "        \"Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos E-O\",\n",
    "        \"Totem 64 Rue de Rivoli Totem 64 Rue de Rivoli Vélos O-E\",\n",
    "    ],\n",
    "    'sources':[\n",
    "        '10 avenue de la Grande Armée SE-NO',\n",
    "        '10 avenue de la Grande Armée SE-NO',\n",
    "        '27 quai de la Tournelle NO-SE',\n",
    "        '27 quai de la Tournelle SE-NO',\n",
    "        \"Face au 48 quai de la marne NE-SO\",\n",
    "        \"Face au 48 quai de la marne SO-NE\",\n",
    "        \"Pont des Invalides (couloir bus) N-S\",\n",
    "        \"Quai des Tuileries NO-SE\",\n",
    "        \"Quai des Tuileries SE-NO\",\n",
    "        \"Totem 64 Rue de Rivoli E-O\",\n",
    "        \"Totem 64 Rue de Rivoli O-E\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "for source, cible in zip(dico_repl['sources'], dico_repl['cibles']):\n",
    "    # Condition source : on prend la première ligne correspondante\n",
    "    cond_source = (df_cpt_velo['nom_du_compteur'] == source)\n",
    "    df_source = df_cpt_velo.loc[cond_source]\n",
    "    print(f\"source [{source}]\\ncible [{cible}]\\nshape source [{df_source.shape}]\")\n",
    "    if df_source.empty:\n",
    "        print(f\"⚠️ Aucun compteur source trouvé pour: {source}\")\n",
    "        continue\n",
    "    # Condition cible : les lignes à modifier\n",
    "    cond_cible = (df_cpt_velo['nom_du_compteur'] == cible)\n",
    "    df_cible = df_cpt_velo.loc[cond_cible]\n",
    "    if df_cible.empty:\n",
    "        print(f\"⚠️ Aucun compteur cible trouvé pour: {cible}\")\n",
    "        continue\n",
    "    display(\"### Contenu utilisé ###\",df_source.iloc[0])\n",
    "    ligne_source = df_source.iloc[0]  # première occurrence\n",
    "    # Affectation des valeurs des colonnes choisies depuis la ligne source vers les lignes cibles\n",
    "    df_cpt_velo.loc[cond_cible, colonnes_a_copier] = ligne_source[colonnes_a_copier].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup after counter names fusion\n",
    "df_cpt_velo_bckp_fusion = df_cpt_velo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df_cpt_velo = df_cpt_velo_bckp_fusion.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Représentation des valeur NA graphiquement\n",
    "msno.matrix(df_cpt_velo_bckp_fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">Après traitement des noms de station seules les colonnes spécifiques au compteur sont encore avec valeurs manquantes et nous décidons de les abandonner\n",
    ">\n",
    ">*[insérer **graphique à jour** cellule ci-dessus]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "#### Suppression des colonnes n'apportant pas de valeur explicative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_suppr = [\n",
    "    'lien_vers_photo_du_site_de_comptage',\n",
    "    'identifiant_technique_compteur',\n",
    "    'id_photos',\n",
    "    'test_lien_vers_photos_du_site_de_comptage',\n",
    "    'id_photo_1',\n",
    "    'date_d_installation_du_site_de_comptage',\n",
    "    'url_sites',\n",
    "    'type_dimage',\n",
    "]\n",
    "df_cpt_velo = df_cpt_velo.drop(columns=col_suppr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup after drop columns\n",
    "df_cpt_velo_bckp_drop = df_cpt_velo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df_cpt_velo = df_cpt_velo_bckp_drop.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_cpt_velo_bckp_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">La suppression des colonnes sans valeur explicative (relative a un compteur unique multipliées X fois) permet de commencer a travailler sur les types et les distributions et correlations des données restantes.\n",
    ">\n",
    ">*[insérer **graphique à jour** cellule ci-dessus]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "#### Traitement identifiant_du_site_de_comptage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.identifiant_du_site_de_comptage = df_cpt_velo.identifiant_du_site_de_comptage.astype(int).astype(str)\n",
    "dfc.display_variable_info(df_cpt_velo.identifiant_du_site_de_comptage)\n",
    "print(df_cpt_velo.identifiant_du_site_de_comptage.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "#### Traitement date_et_heure_de_comptage et extraction des données cycliques sur cette date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo = dfps.extract_datetime_features(\n",
    "    df_cpt_velo,\n",
    "    timestamp_col=\"date_et_heure_de_comptage\",\n",
    "    tz_local=\"Europe/Paris\"\n",
    ")\n",
    "df_cpt_velo.info()\n",
    "df_cpt_velo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Extraction latitude/longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo[['latitude', 'longitude']] = df_cpt_velo['coordonnees_geographiques'].str.split(',', expand=True).astype(float)\n",
    "dfc.display_variable_info(df_cpt_velo[['latitude', 'longitude']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "#### Traitement des valeurs aberrantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.loc[df_cpt_velo.comptage_horaire>3000, 'comptage_horaire'] /= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Ajout de l'arrondissement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrondissements_gdf = dfps.load_communes_from_config('communes_geo_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_communes = arrondissements_gdf[\n",
    "    arrondissements_gdf[\"code\"].str.startswith((\"75\", \"92\", \"93\", \"94\", \"77\", \"78\", \"91\", \"95\"))\n",
    "].copy()\n",
    "paris_arrondissements = idf_communes[idf_communes[\"commune\"] == \"75056\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo = dfps.assign_communes_to_df(df_cpt_velo, 'longitude', 'latitude', paris_arrondissements, 'nom', 'arrondissement')\n",
    "df_cpt_velo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Sauvegarde après features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backup after feature management\n",
    "df_cpt_velo_bckp_feats = df_cpt_velo.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore (if needed to recover)\n",
    "df_cpt_velo = df_cpt_velo_bckp_feats.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "#### Sauvegarde du dataset en CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.to_csv(\"comptage-velo-donnees-compteurs_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Visualisation comptage_horaire sur données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
    "\n",
    "# Premier graphique\n",
    "sns.ecdfplot(\n",
    "    data=df_cpt_velo_bckp_orig,\n",
    "    x=df_cpt_velo_bckp_orig.comptage_horaire.name, # type: ignore[reportArgumentType]\n",
    "    ax=ax1\n",
    ")\n",
    "quantiles = df_cpt_velo_bckp_orig.comptage_horaire.quantile([0.8, 0.99])\n",
    "for prob, val in quantiles.items():\n",
    "    ax1.annotate(\n",
    "        f\"comptage_horaire < {int(val)}\\n({int(prob*100)}% des données)\",  # type: ignore\n",
    "        xy=(val, prob),\n",
    "        xytext=(val + 500, prob - 0.2),  # type: ignore\n",
    "        arrowprops=dict(arrowstyle=\"->\", color='red'),\n",
    "        fontsize=10,\n",
    "        color='red',\n",
    "        ha='left'\n",
    "    )\n",
    "ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "               ['0', '20%', '40%', '60%', '80%', '100%'])\n",
    "ax1.set_title(\"Distribution cumulée des valeurs de comptage horaire\")\n",
    "# Deuxième graphique (regroupement par mois)\n",
    "sns.boxplot(\n",
    "    data=df_cpt_velo_bckp_orig,\n",
    "    x=\"mois_annee_comptage\",\n",
    "    y=\"comptage_horaire\",\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Répartition des comptages horaires par mois\")\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "# Détection manuelle ou dynamique de l'outlier\n",
    "x_pos = df_cpt_velo_bckp_orig['mois_annee_comptage'].unique().tolist().index('2025-01')  # Position de '2025-01'\n",
    "y_pos = df_cpt_velo_bckp_orig[df_cpt_velo_bckp_orig['mois_annee_comptage'] == '2025-01']['comptage_horaire'].max()  # Valeur max du mois\n",
    "# Annotation avec flèche\n",
    "ax2.annotate(\n",
    "    'Affluence aberrante\\nle 05-01-2025 à 14h\\ncompteur \"Quai d\\'Orsay O-E\"',\n",
    "    xy=(x_pos, y_pos),\n",
    "    xytext=(x_pos - 3, y_pos - 800),  # Position du texte\n",
    "    arrowprops=dict(facecolor='red', arrowstyle='->'),\n",
    "    fontsize=10,\n",
    "    color='red',\n",
    "    ha='left'\n",
    ")\n",
    "# Titre global\n",
    "fig.suptitle(\n",
    "    'Analyse des comptages horaires',\n",
    "    fontsize=20,\n",
    "    y=1.02\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "#### Visualisation comptage_horaire après feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(12, 10))\n",
    "\n",
    "# Premier graphique\n",
    "sns.ecdfplot(\n",
    "    data=df_cpt_velo_bckp_feats,\n",
    "    x=df_cpt_velo_bckp_feats.comptage_horaire.name, # type: ignore[reportArgumentType]\n",
    "    ax=ax1\n",
    ")\n",
    "quantiles = df_cpt_velo_bckp_feats.comptage_horaire.quantile([0.8, 0.99])\n",
    "for prob, val in quantiles.items():\n",
    "    ax1.annotate(\n",
    "        f\"comptage_horaire < {int(val)}\\n({int(prob*100)}% des données)\",  # type: ignore\n",
    "        xy=(val, prob),\n",
    "        xytext=(val + 100, prob - 0.2),  # type: ignore\n",
    "        arrowprops=dict(arrowstyle=\"->\", color='red'),\n",
    "        fontsize=10,\n",
    "        color='red',\n",
    "        ha='left'\n",
    "    )\n",
    "ax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.0],\n",
    "               ['0', '20%', '40%', '60%', '80%', '100%'])\n",
    "ax1.set_title(\"Distribution cumulée des valeurs de comptage horaire\")\n",
    "# Deuxième graphique (regroupement par mois)\n",
    "sns.boxplot(\n",
    "    data=df_cpt_velo_bckp_feats,\n",
    "    x=\"mois_annee_comptage\",\n",
    "    y=\"comptage_horaire\",\n",
    "    ax=ax2\n",
    ")\n",
    "ax2.set_title(\"Répartition des comptages horaires par mois\")\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "# Titre global\n",
    "fig.suptitle(\n",
    "    'Analyse des comptages horaires après modification de la valeur aberrante',\n",
    "    fontsize=20,\n",
    "    y=1.02\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cpt_velo_bckp_orig[df_cpt_velo_bckp_orig.comptage_horaire>3000])\n",
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 5))\n",
    "sns.boxplot(\n",
    "    data=df_cpt_velo_bckp_orig[(df_cpt_velo_bckp_orig.nom_du_compteur==\"Quai d'Orsay O-E\")\n",
    "                     &(df_cpt_velo_bckp_orig.mois_annee_comptage==\"2025-01\")],\n",
    "    x=\"comptage_horaire\", \n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title(\"Répartition des comptages horaires pour Janvier 2025 Quai d'Orsay\")\n",
    "# Titre global\n",
    "fig.suptitle(\n",
    "    'Analyse valeur abérrante',\n",
    "    fontsize=20,\n",
    "    y=1.02\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(nrows=1, ncols=1, figsize=(12, 5))\n",
    "sns.boxplot(\n",
    "    data=df_cpt_velo[(df_cpt_velo.nom_du_compteur==\"Quai d'Orsay O-E\")\n",
    "                     &(df_cpt_velo.mois_annee_comptage==\"2025-01\")],\n",
    "    x=\"comptage_horaire\", \n",
    "    ax=ax1\n",
    ")\n",
    "ax1.set_title(\"Répartition des comptages horaires pour Janvier 2025 Quai d'Orsay\")\n",
    "# Titre global\n",
    "fig.suptitle(\n",
    "    'Distribution après ajustement de la valeur abérrante',\n",
    "    fontsize=20,\n",
    "    y=1.02\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">La distribution des valeurs de la variable comptage_horaire (que ce soit globalement ou par mois) est fortement concentrée sur les faibles valeurs avec beaucoup de valeurs extrêmes mais homogènement distribuées.\n",
    ">\n",
    ">Une seule valeur aberrante fait état de 3070 vélos comptés sur la seule tranche horaire de 14h le 5 janvier 2025 sur le compteur \"Quai d'Orsay O-E\". On peut visualiser avec la distribution du comptage par heure sur ce site pour la journée associée que c'est une valeur de magnitude x10 par rapport aux autres, nous la corrigeons dans ce sens.\n",
    ">\n",
    ">Le comptage des bornes vélib dans la zone pourrait être une source exogène interessante (même si là encore difficile à relier à notre dataset).\n",
    ">On pourrait imaginer créer une séquence de X nouvelles variables pour chaque ligne avec les X compteurs Vélib les plus proches\n",
    ">\n",
    ">Il pourrait aussi être intéressant d'ajouter un historique plus profond que les 13 derniers mois\n",
    ">\n",
    ">*[insérer **graphique à jour** cellule ci-dessus]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "#### Normal distribution visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "# Liste des colonnes numériques\n",
    "numeric_cols = ['comptage_horaire']\n",
    "\n",
    "# Paramètres de layout\n",
    "n_rows = len(numeric_cols)\n",
    "fig_height_per_plot = 8  # Hauteur fixe par subplot\n",
    "fig_width = 15  # Largeur large (plein écran typique)\n",
    "\n",
    "# Crée la grille de subplots\n",
    "fig, axes = plt.subplots(n_rows, 1, figsize=(fig_width, n_rows * fig_height_per_plot))\n",
    "\n",
    "# Si 1 seul subplot, axes n'est pas une liste — on force la conversion\n",
    "if n_rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Génère chaque QQ-plot dans sa case\n",
    "for ax, col in zip(axes, numeric_cols):\n",
    "    (osm, osr), (slope, intercept, r) = probplot(df_cpt_velo_bckp_orig[col], dist=\"norm\")\n",
    "    # Paramètre : seuil de tolérance d'écart entre donnée et droite normale 15% de l'écart type réel\n",
    "    std_real = df_cpt_velo_bckp_orig[col].std()\n",
    "    tolerance = 0.15 * std_real\n",
    "    # Valeurs attendues selon la droite théorique\n",
    "    expected = slope * osm + intercept  # type: ignore \n",
    "    # Écart absolu entre les données réelles triées et les valeurs théoriques\n",
    "    residuals = np.abs(osr - expected)\n",
    "    # Points jugés \"normaux\"\n",
    "    normal_like_indices = residuals < tolerance\n",
    "    n_total = len(osr)\n",
    "    n_normal_like = normal_like_indices.sum()\n",
    "    percentage_normal = 100 * n_normal_like / n_total\n",
    "    # intervalle en sigma\n",
    "    min_sigma = osm[normal_like_indices].min()\n",
    "    max_sigma = osm[normal_like_indices].max()\n",
    "    # Affichage du QQ-plot \n",
    "    probplot(df_cpt_velo_bckp_orig[col], dist=\"norm\", plot=ax)\n",
    "    # Surlignage des points normaux en vert\n",
    "    ax.plot(osm[normal_like_indices], osr[normal_like_indices], 'go', label='≈ Normale (seuil de 15% Vs l\\'écart type réel)')\n",
    "    # Titre et légende\n",
    "    ax.set_title(\n",
    "        f\"QQ-Plot - {col}\\n\"\n",
    "        f\"Plage ≈ normale : {min_sigma:.2f}σ à {max_sigma:.2f}σ | \"\n",
    "        f\"{percentage_normal:.1f}% des valeurs\"\n",
    "    )\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique après ajustement features\n",
    "# Liste des colonnes numériques\n",
    "numeric_cols = ['comptage_horaire']\n",
    "# numeric_cols = df_cpt_velo_feats.select_dtypes(include=\"number\").columns\n",
    "\n",
    "# Paramètres de layout\n",
    "n_rows = len(numeric_cols)\n",
    "fig_height_per_plot = 8  # Hauteur fixe par subplot\n",
    "fig_width = 15  # Largeur large (plein écran typique)\n",
    "\n",
    "# Crée la grille de subplots\n",
    "fig, axes = plt.subplots(n_rows, 1, figsize=(fig_width, n_rows * fig_height_per_plot))\n",
    "\n",
    "# Si 1 seul subplot, axes n'est pas une liste — on force la conversion\n",
    "if n_rows == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Génère chaque QQ-plot dans sa case\n",
    "for ax, col in zip(axes, numeric_cols):\n",
    "    (osm, osr), (slope, intercept, r) = probplot(df_cpt_velo_bckp_feats[col], dist=\"norm\")\n",
    "    # Paramètre : seuil de tolérance d'écart entre donnée et droite normale 15% de l'écart type réel\n",
    "    std_real = df_cpt_velo_bckp_feats[col].std()\n",
    "    tolerance = 0.15 * std_real\n",
    "    # Valeurs attendues selon la droite théorique\n",
    "    expected = slope * osm + intercept  # type: ignore \n",
    "    # Écart absolu entre les données réelles triées et les valeurs théoriques\n",
    "    residuals = np.abs(osr - expected)\n",
    "    # Points jugés \"normaux\"\n",
    "    normal_like_indices = residuals < tolerance\n",
    "    n_total = len(osr)\n",
    "    n_normal_like = normal_like_indices.sum()\n",
    "    percentage_normal = 100 * n_normal_like / n_total\n",
    "    # intervalle en sigma\n",
    "    min_sigma = osm[normal_like_indices].min()\n",
    "    max_sigma = osm[normal_like_indices].max()\n",
    "    # Affichage du QQ-plot \n",
    "    probplot(df_cpt_velo_bckp_feats[col], dist=\"norm\", plot=ax)\n",
    "    # Surlignage des points normaux en vert\n",
    "    ax.plot(osm[normal_like_indices], osr[normal_like_indices], 'go', label='≈ Normale (seuil de 15% Vs l\\'écart type réel)')\n",
    "    # Titre et légende\n",
    "    ax.set_title(\n",
    "        f\"QQ-Plot - {col}\\n\"\n",
    "        f\"Plage ≈ normale : {min_sigma:.2f}σ à {max_sigma:.2f}σ | \"\n",
    "        f\"{percentage_normal:.1f}% des valeurs\"\n",
    "    )\n",
    "    ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">La répartition des données de comptage ne suit globalement pas une loi normale, avant ou après modification de la valeur aberrante\n",
    ">\n",
    ">*[insérer **graphique à jour** cellule ci-dessus]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regroupement par site\n",
    "df_grouped = df_cpt_velo.groupby(['latitude', 'longitude', 'arrondissement'], as_index=False)['comptage_horaire'].sum()\n",
    "df_grouped = df_grouped.rename(columns={'comptage_horaire': 'comptage_total'})  # type: ignore \n",
    "\n",
    "# Affichage avec plotly express (scatter_map)\n",
    "fig_ct = px.scatter_map(\n",
    "    df_grouped,\n",
    "    lat='latitude',\n",
    "    lon='longitude',\n",
    "    size='comptage_total',\n",
    "    labels='arrondissement',\n",
    "    size_max=40,\n",
    "    zoom=12,\n",
    "    center={'lat': 48.8566, 'lon': 2.3522},  # Centré sur Paris\n",
    "    hover_name='arrondissement',\n",
    "    title=\"Comptage total par site (Paris)\",\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig_ct.update_layout(margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0})\n",
    "fig_ct.show()\n",
    "fig_ct.write_html(\"comptage_total_par_site_geomap.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_fr = {\n",
    "    0: 'Dimanche',\n",
    "    1: 'Lundi',\n",
    "    2: 'Mardi',\n",
    "    3: 'Mercredi',\n",
    "    4: 'Jeudi',\n",
    "    5: 'Vendredi',\n",
    "    6: 'Samedi'\n",
    "}\n",
    "df_cpt_velo['jour_semaine'] = df_cpt_velo['date_et_heure_de_comptage_day_of_week'].map(jours_fr)\n",
    "fig_agg1 = px.sunburst(\n",
    "    df_cpt_velo,\n",
    "    path=['arrondissement', 'jour_semaine', 'date_et_heure_de_comptage_hour'], \n",
    "    values='comptage_horaire',\n",
    "    color='arrondissement',\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "fig_agg1.show()\n",
    "fig_agg1.write_html(\"comptage_total_par_arr_jour-sem_heure.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_fr = {\n",
    "    0: 'Dimanche',\n",
    "    1: 'Lundi',\n",
    "    2: 'Mardi',\n",
    "    3: 'Mercredi',\n",
    "    4: 'Jeudi',\n",
    "    5: 'Vendredi',\n",
    "    6: 'Samedi'\n",
    "}\n",
    "df_cpt_velo['jour_semaine'] = df_cpt_velo['date_et_heure_de_comptage_day_of_week'].map(jours_fr)\n",
    "fig_agg2 = px.sunburst(\n",
    "    df_cpt_velo,\n",
    "    path=['arrondissement', 'nom_du_site_de_comptage', 'date_et_heure_de_comptage_hour'], \n",
    "    values='comptage_horaire',\n",
    "    color='arrondissement',\n",
    "    width=1200,\n",
    "    height=1000,\n",
    ")\n",
    "fig_agg2.show()\n",
    "fig_agg2.write_html(\"comptage_total_par_arr_nom-site_heure.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">La relation entre coordonnées géographique (visualisées sur une carte) et le nombre de vélos totals observés sur cette position géographique\n",
    "pourrait être un axe intéressant à étudier sur la base de critères de regroupement (clusterisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "\n",
    "## 3.6 Analyse statistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderson :\n",
    "# Hypothèse nulle H0 : -> la distribution est normale\n",
    "# Hypothèse alternative H1 : -> on réfute la distribution normale\n",
    "# Explication du test : si pour une tolérance donnée, la statistique de test est supérieure au seuil critique alors on rejette \n",
    "# statistiquement H0 et on accepte H1 sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "\n",
    "# ici tous les seuil même les plus large (15%) rejette l'hypothese de normalité (ce qu'on voyait déjà à l'oeil nu graphiquement)\n",
    "result = anderson(df_cpt_velo['comptage_horaire'])\n",
    "print(f\"Statistique de test = {result.statistic:.4f}\")\n",
    "for i in range(len(result.critical_values)):\n",
    "    sig_level = result.significance_level[i]\n",
    "    crit_value = result.critical_values[i]\n",
    "    if result.statistic > crit_value:\n",
    "        print(f\"❌ À {sig_level}% : rejet de la normalité (stat > seuil critique {crit_value:.3f})\")\n",
    "    else:\n",
    "        print(f\"✅ À {sig_level}% : pas de preuve contre la normalité\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "#### Pearson (quantitatives against quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson\n",
    "# Hypothèse nulle H0 : -> les deux variables quantitatives ne sont pas correlées (corrélation est nulle)\n",
    "# Hypothèse alternative H1 : -> il existe une correlation (corrélation <> nulle)\n",
    "# Explication du test : si la statistique de test a une p-valeur inférieure à un seuil de tolérance (0,05) alors on rejette \n",
    "# statistiquement H0 et on accepte H1 (il existe une correlation) sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "coeff_corr, p_valeur = pearsonr (x=df_cpt_velo['comptage_horaire'], y=df_cpt_velo['latitude'])\n",
    "print(f\"comptage_horaire/latitude: coefficient de correlation[{coeff_corr}] et p-valeur[{p_valeur}]\")\n",
    "coeff_corr, p_valeur = pearsonr (x=df_cpt_velo['comptage_horaire'], y=df_cpt_velo['longitude'])\n",
    "print(f\"comptage_horaire/longitude: coefficient de correlation[{coeff_corr}] et p-valeur[{p_valeur}]\")\n",
    "corr_matrix = df_cpt_velo.select_dtypes(include='number').corr(method='pearson')\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", mask=np.triu(corr_matrix))\n",
    "plt.title(\"Matrice de corrélation (Pearson)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "#### ANOVA (quantitative against qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (NB : a priori le test ANOVA n'est pas robuste si notre variable quantitative ne suit pas une loi normale)\n",
    "# Hypothèse nulle H0 : -> pas d'effet significatif de la variable qualitative sur la variable quantitative\n",
    "# Hypothèse alternative H1 : -> il y a un effet significatif de la variable qualitative sur la variable quantitative\n",
    "# Explication du test : si la statistique de test a une p-valeur inférieure a un seuil de tolérance (0,05) alors on rejette \n",
    "# statistiquement H0 et on accepte H1 (il y a un effet significatif de la variable qualitative sur la variable quantitative) \n",
    "# sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "result = smf.ols('comptage_horaire ~ arrondissement + orientation_compteur + mois_annee_comptage', data=df_cpt_velo).fit()\n",
    "display(sm.stats.anova_lm(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse des coefficients du modèle\n",
    "# - Condition Number ≈ 33.8 : faible → pas de problème de multicolinéarité (et plus de message sur la singularité de la matrice)\n",
    "# - Tous les coefficients sont raisonnables en magnitude et en interprétation\n",
    "# - Toutes les modalités de arrondissement, orientation_compteur, mois_annee_comptage ont des p-valeurs < 0.001, \n",
    "# sauf un mois (2024-04 non significatif à 5 %)\n",
    "# - Les F-statistiques de l’ANOVA sont élevées → chaque variable apporte significativement à l’explication de la variance de \n",
    "# comptage_horaire\n",
    "# R² = 0.230 → le modèle explique 23 % de la variance du comptage horaire\n",
    "# C’est modeste, mais totalement attendu pour ce type de phénomène complexe, dépendant aussi de la météo, des événements, \n",
    "# des jours fériés, emplacement Velib\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "#### CHI2 (qualitative against qualitative) - out of scope for the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHI2\n",
    "# Hypothèse nulle H0 : -> la variable qualitative A est indépendante de la variable qualitative B\n",
    "# Hypothèse alternative H1 : -> la variable qualitative A n'est pas indépendante de la variable qualitative B\n",
    "# Explication du test : si la statistique de test a une p-valeur inférieure a un seuil de tolérance (0,05) alors on rejette \n",
    "# statistiquement H0 et on accepte H1 (la variable qualitative A n'est pas indépendante de la variable qualitative B)\n",
    "# sinon on n'a PAS de preuve statistique contre H0 et on ne peut rien conclure\n",
    "contingence_arr_orient = pd.crosstab(df_cpt_velo[\"arrondissement\"], df_cpt_velo[\"mois_annee_comptage\"])\n",
    "display(contingence_arr_orient)\n",
    "chi2, pvalue = chi2_contingency(observed=contingence_arr_orient)[0:2]\n",
    "alpha = 0.05\n",
    "if pvalue < alpha:\n",
    "    print(\"Le test chi2 =\", \n",
    "          chi2,\n",
    "          \"avec p-value\", \n",
    "          pvalue,\n",
    "          \"permet de refuter H0 et donc d'affirmer la vraissemblance de H1 (il y a correlation entre les deux variables)\")\n",
    "else:\n",
    "    print(\"Le test chi2 =\", \n",
    "          chi2,\n",
    "          \"avec p-value\", \n",
    "          pvalue,\n",
    "          \"ne permet pas de refuter H0 on ne peut pas conclure\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
