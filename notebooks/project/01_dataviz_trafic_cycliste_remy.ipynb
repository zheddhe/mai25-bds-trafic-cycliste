{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9683e0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dbc338",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# transformation\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# resampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# metrics and evaluation\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from scipy.stats import chi2_contingency, probplot\n",
    "from xgboost import plot_importance\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# # graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# # for jupyter notebook display management\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce0835",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3ddc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5054b",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a20d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b773e7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade53e8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be26f9c",
   "metadata": {},
   "source": [
    "### 2.1.1 VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4f75d",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a4bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo_raw = dfc.load_dataset_from_config('velo_comptage_data', sep=';')\n",
    "\n",
    "if df_cpt_velo_raw is not None and isinstance(df_cpt_velo_raw, pd.DataFrame):\n",
    "    display(df_cpt_velo_raw.head())\n",
    "    dfc.log_general_info(df_cpt_velo_raw)\n",
    "    nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_cpt_velo_raw)\n",
    "    if nb_first != nb_total:\n",
    "        print(dfc.duplicates_index_map(df_cpt_velo_raw))\n",
    "    df_cpt_velo = dfc.normalize_column_names(df_cpt_velo_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50365e0c",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.info()\n",
    "display(df_cpt_velo.head())\n",
    "df_cpt_velo_desc = df_cpt_velo.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_cpt_velo.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc17a6a",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfff311",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4702007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and duplicates management\n",
    "df_cpt_velib_orig = df_cpt_velo.copy()\n",
    "df_cpt_velo = df_cpt_velo.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecadc44",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0aa23",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b764832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "for col in df_cpt_velo.select_dtypes(include='number').columns:\n",
    "    probplot(df_cpt_velo[col], dist=\"norm\", plot=plt)\n",
    "    plt.suptitle(f\"Column {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6ea89",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3b2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c09e68df",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6566ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1415bcaa",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f6b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00984daa",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b735c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3de3e447",
   "metadata": {},
   "source": [
    "# 4. Division in Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temporaire) Ajustement : enlever les variables non retravaillées pour le moment\n",
    "df_disp_velib = df_cpt_velo.drop(columns=['identifiant_station',\n",
    "                                            'nom_station', \n",
    "                                            'actualisation_de_la_donnee', \n",
    "                                            'coordonnees_geographiques', \n",
    "                                            'nom_communes_equipees',\n",
    "                                            'code_insee_communes_equipees',\n",
    "                                            'station_opening_hours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcde203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features (X) et target (y) pour train et test\n",
    "target_col = 'station_en_fonctionnement'\n",
    "features = df_cpt_velo.drop(target_col, axis=1)\n",
    "target = df_cpt_velo[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=66)\n",
    "print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set (X/y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7de26",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "Règle d'or : Toute opération qui \"apprend\" des données (i.e. utilise l’ensemble des valeurs pour calculer quelque chose) doit être faite après le split train/test — c’est-à-dire uniquement sur le train.\n",
    "\n",
    "| Type de transformation                                                                                    | À faire avant le split ?                    | Détails                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ Création de features basées sur les colonnes existantes (ex: `BMI = weight / height²`)                  | **Avant**                                   | Pas de risque de fuite car c’est purement déterministe.            |\n",
    "| ⚠️ Calculs dépendant de la distribution (moyennes, encodage fréquentiel, imputation par la médiane, etc.) | **Après** (sur le train uniquement)         | Risque de fuite de données si appliqué sur l’ensemble avant split. |\n",
    "| ✅ Ajout de features exogènes fixes (données météo, géographiques, calendaires, etc.)                      | **Avant**                                   | Pas de dépendance au `target` ni à la répartition train/test.      |\n",
    "| ⚠️ Encoding (`LabelEncoder`, `OneHot`, `TargetEncoding`, etc.)                                            | **Fit sur train, transform sur train/test** | Toujours fitter uniquement sur le `train`.                         |\n",
    "| ⚠️ Standardisation / normalisation (Scaler)                                                               | **Fit sur train, transform sur train/test** | Pareil : `.fit()` sur train, `.transform()` sur test.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ccbe0e",
   "metadata": {},
   "source": [
    "## 5.1 Modification localisées sur les variables d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d960be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification localisée en fonction de la proximité à la médiane d'autre variables\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "#     (abs(train['Age'] - 30) > abs(train['Age'] - 41)) & # L’âge est plus proche de 41 que de 30\n",
    "#     (train['Previously_Insured'] == 0) & # La personne n’était pas assurée auparavant\n",
    "#     (train['Vehicle_Damage'] == 1) # Elle a subi un dommage sur son véhicule\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e34364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification par répartition spécifique entre deux valeurs 0 et 1\n",
    "# proportion_tab = [0] * 55 + [1] * 45\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = train.loc[mask, 'Gender'].apply(lambda x: random.choice(proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aae4ef",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc760a2",
   "metadata": {},
   "source": [
    "### 5.2.1 Scaling (données quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ni outlier ni distribution loi normale : min/max\n",
    "# - sans outlier mais distribution loi normale : standard\n",
    "# - avec outlier : Robust \n",
    "mm_scal = MinMaxScaler()\n",
    "r_scal = RobustScaler()\n",
    "s_scal = StandardScaler()\n",
    "\n",
    "r_scal_col = ['']\n",
    "df_disp_velib[r_scal_col] = s_scal.fit_transform(df_disp_velib[r_scal_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d626f4a",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoding (données qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e514112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique                 Type                Colonnes créées     Principe\n",
    "# get_dummies()\t            Nominale\t        N (ou N–1)\t        Binaire par modalité\n",
    "# OneHotEncoder\t            Nominale, Cyclique\tN\t                Colonne 0/1 par modalité\n",
    "# Sum Encoding\t            Nominale\t        N–1\t                Différence avec moyenne globale\n",
    "# Helmert Encoding\t        Nominale\t        N–1\t                Contraste avec moyenne des modalités précédentes\n",
    "# Backward Difference\t    Ordinale\t        N–1\t                Contraste avec moyenne des modalités suivantes\n",
    "# Binary Encoding\t        Nominale\t        log₂(N)\t            Encodage binaire de l’index\n",
    "# Hashing Encoding\t        Nominale\t        n_components\t    Hash des modalités sur colonnes fixes\n",
    "# Label Encoding\t        Ordinale\t        1\t                Entier arbitraire\n",
    "# Ordinal Encoding\t        Ordinale\t        1\t                Rang croissant des modalités\n",
    "# Target Encoding\t        Nominale/Ordinale\t1\t                Moyenne de la cible par modalité\n",
    "# Mean Encoding\t            Nominale/Ordinale\t1\t                Idem Target Encoding\n",
    "# Frequency Encoding\t    Nominale/Ordinale\t1\t                Fréquence d'apparition\n",
    "# Leave-One-Out\t            Nominale/Ordinale\t1\t                Moyenne de la cible, sauf ligne courante\n",
    "# James-Stein Encoding\t    Nominale/Ordinale\t1\t                Moyenne pondérée par variance intercatégorie\n",
    "# M-Estimate Encoding\t    Nominale/Ordinale\t1\t                Moyenne cible lissée vers moyenne globale\n",
    "# Probability Ratio\t        Ordinale, binaire\t1\t                Log du ratio de probas classe 1 / classe 0\n",
    "# WOE Encoding\t            Ordinale, binaire\t1\t                Log( %positif / %négatif )\n",
    "# Thermometer Encoding\t    Ordinale\t        N\t                1 si la modalité est ≤ à une valeur\n",
    "# Trigonométrique (sin/cos)\tCyclique\t        2\t                Encode la cyclicité\n",
    "# Fourier / Radial\t        Cyclique\t        Variable\t        Approximation périodique (base)\n",
    "ohe_enc_col = ['']\n",
    "ohe_enc = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "# Appliquer OneHotEncoder\n",
    "X_train_enc_cat = ohe_enc.fit_transform(X_train[ohe_enc_col])\n",
    "X_test_enc_cat = ohe_enc.transform(X_test[ohe_enc_col])\n",
    "# Ajout des colonnes encodées à un DataFrame car le resultat de enc.fit/transform estun ndarray sans index/colonnes\n",
    "X_train_cat_df = pd.DataFrame(X_train_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_train.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_test.index)\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train = pd.concat([X_train.drop(columns=ohe_enc_col), X_train_cat_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=ohe_enc_col), X_test_cat_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
