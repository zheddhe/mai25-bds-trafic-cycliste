{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# 1. Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data management\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "### Machine Learning\n",
    "\n",
    "# transformation\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder\n",
    "\n",
    "# models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# metrics and evaluation\n",
    "from scipy.stats import probplot\n",
    "\n",
    "### Data Viz\n",
    "\n",
    "# graphical basics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# graphical seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "# # graphical plotly\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# # for jupyter notebook display management\n",
    "# import plotly.io as pio\n",
    "# pio.renderers.default = \"notebook\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 General dataframe functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.dataframe_common as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 General Classification functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import smartcheck.classification_common as cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# 2. Loading and Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Loading of data sets and general exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### 2.1.1 VELO COMPTAGE (Main Data Set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Loading and column management (columns names normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo_raw = dfc.load_dataset_from_config('velo_comptage_data', sep=';')\n",
    "\n",
    "if df_cpt_velo_raw is not None and isinstance(df_cpt_velo_raw, pd.DataFrame):\n",
    "    df_cpt_velo = dfc.normalize_column_names(df_cpt_velo_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">On normalise les noms de colonnes du data set afin de faciliter leur manipulation dans le code. La norme utilisée est de type **snake_case**\n",
    ">  - Remplacement des caractères spéciaux et espaces par des underscores\n",
    ">  - Conversion en minuscules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_cpt_velo.head())\n",
    "dfc.log_general_info(df_cpt_velo)\n",
    "nb_first, nb_total = dfc.detect_and_log_duplicates_and_missing(df_cpt_velo)\n",
    "if nb_first != nb_total:\n",
    "    print(dfc.duplicates_index_map(df_cpt_velo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    ">**Rapport**\n",
    ">\n",
    ">Le dataset ***comptage velo*** présente globalement **942554** observations (lignes) sur **16** variables (colonnes)\n",
    ">  - Il ne possède aucune observation en doublon ou sans aucune information.\n",
    ">  - La proportion d'informations manquantes (détail par variable ci-dessous) est 3.38%\n",
    ">\n",
    ">| Variable | % informations manquantes |\n",
    ">|-|-|\n",
    ">| identifiant_du_compteur | 3.98% |\n",
    ">| identifiant_du_site_de_comptage | 3.98% |\n",
    ">| nom_du_site_de_comptage | 3.98% |\n",
    ">| date_d_installation_du_site_de_comptage | 3.98% |\n",
    ">| lien_vers_photo_du_site_de_comptage | 5.02% |\n",
    ">| coordonnees_geographiques | 3.98% |\n",
    ">| identifiant_technique_compteur | 5.02% |\n",
    ">| id_photos | 5.02% |\n",
    ">| test_lien_vers_photos_du_site_de_comptage | 5.02% |\n",
    ">| id_photo_1 | 5.02% |\n",
    ">| url_sites | 3.98% |\n",
    ">| type_dimage | 5.02% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Global description and correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cpt_velo.info()\n",
    "display(df_cpt_velo.head())\n",
    "df_cpt_velo_desc = df_cpt_velo.select_dtypes(include=np.number).describe()\n",
    "display(df_cpt_velo_desc)\n",
    "df_cpt_velo_cr = df_cpt_velo.select_dtypes(include=np.number).corr()\n",
    "display(df_cpt_velo_cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## 2.2 Data quality refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 2.2.1 VELO COMPTAGE (Main Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original backup and duplicates management\n",
    "df_cpt_velib_orig = df_cpt_velo.copy()\n",
    "df_cpt_velo = df_cpt_velo.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "# 3. Data Viz' and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## 3.1 General Data Viz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérificationn graphique de la répartition en loi normale de chaque données numérique\n",
    "for col in df_cpt_velo.select_dtypes(include='number').columns:\n",
    "    probplot(df_cpt_velo[col], dist=\"norm\", plot=plt)\n",
    "    plt.suptitle(f\"Column {col}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 3.2 Quantitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 3.3 Qualitative mono variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 3.4 Qualitative multi variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 3.5 Quantitative multi variable correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "# 4. Division in Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_disp_velib = df_disp_velib_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (temporaire) Ajustement : enlever les variables non retravaillées pour le moment\n",
    "df_disp_velib = df_cpt_velo.drop(columns=['identifiant_station',\n",
    "                                            'nom_station', \n",
    "                                            'actualisation_de_la_donnee', \n",
    "                                            'coordonnees_geographiques', \n",
    "                                            'nom_communes_equipees',\n",
    "                                            'code_insee_communes_equipees',\n",
    "                                            'station_opening_hours'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation features (X) et target (y) pour train et test\n",
    "target_col = 'station_en_fonctionnement'\n",
    "features = df_cpt_velo.drop(target_col, axis=1)\n",
    "target = df_cpt_velo[target_col]\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=66)\n",
    "print(\"Train Set (X/y):\", X_train.shape, y_train.shape)\n",
    "print(\"Test Set (X/y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# 5. Feature engineering\n",
    "Règle d'or : Toute opération qui \"apprend\" des données (i.e. utilise l’ensemble des valeurs pour calculer quelque chose) doit être faite après le split train/test — c’est-à-dire uniquement sur le train.\n",
    "\n",
    "| Type de transformation                                                                                    | À faire avant le split ?                    | Détails                                                            |\n",
    "| --------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ------------------------------------------------------------------ |\n",
    "| ✅ Création de features basées sur les colonnes existantes (ex: `BMI = weight / height²`)                  | **Avant**                                   | Pas de risque de fuite car c’est purement déterministe.            |\n",
    "| ⚠️ Calculs dépendant de la distribution (moyennes, encodage fréquentiel, imputation par la médiane, etc.) | **Après** (sur le train uniquement)         | Risque de fuite de données si appliqué sur l’ensemble avant split. |\n",
    "| ✅ Ajout de features exogènes fixes (données météo, géographiques, calendaires, etc.)                      | **Avant**                                   | Pas de dépendance au `target` ni à la répartition train/test.      |\n",
    "| ⚠️ Encoding (`LabelEncoder`, `OneHot`, `TargetEncoding`, etc.)                                            | **Fit sur train, transform sur train/test** | Toujours fitter uniquement sur le `train`.                         |\n",
    "| ⚠️ Standardisation / normalisation (Scaler)                                                               | **Fit sur train, transform sur train/test** | Pareil : `.fit()` sur train, `.transform()` sur test.              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 5.1 Modification localisées sur les variables d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification localisée en fonction de la proximité à la médiane d'autre variables\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "#     (abs(train['Age'] - 30) > abs(train['Age'] - 41)) & # L’âge est plus proche de 41 que de 30\n",
    "#     (train['Previously_Insured'] == 0) & # La personne n’était pas assurée auparavant\n",
    "#     (train['Vehicle_Damage'] == 1) # Elle a subi un dommage sur son véhicule\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple de modification par répartition spécifique entre deux valeurs 0 et 1\n",
    "# proportion_tab = [0] * 55 + [1] * 45\n",
    "# mask = (\n",
    "#     (train['Gender'].isna()) &\n",
    "# )\n",
    "# train.loc[mask, 'Gender'] = train.loc[mask, 'Gender'].apply(lambda x: random.choice(proportion))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 5.2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### 5.2.1 Scaling (données quantitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - ni outlier ni distribution loi normale : min/max\n",
    "# - sans outlier mais distribution loi normale : standard\n",
    "# - avec outlier : Robust \n",
    "mm_scal = MinMaxScaler()\n",
    "r_scal = RobustScaler()\n",
    "s_scal = StandardScaler()\n",
    "\n",
    "r_scal_col = ['']\n",
    "df_disp_velib[r_scal_col] = s_scal.fit_transform(df_disp_velib[r_scal_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### 5.2.1 Encoding (données qualitatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technique                 Type                Colonnes créées     Principe\n",
    "# get_dummies()\t            Nominale\t        N (ou N–1)\t        Binaire par modalité\n",
    "# OneHotEncoder\t            Nominale, Cyclique\tN\t                Colonne 0/1 par modalité\n",
    "# Sum Encoding\t            Nominale\t        N–1\t                Différence avec moyenne globale\n",
    "# Helmert Encoding\t        Nominale\t        N–1\t                Contraste avec moyenne des modalités précédentes\n",
    "# Backward Difference\t    Ordinale\t        N–1\t                Contraste avec moyenne des modalités suivantes\n",
    "# Binary Encoding\t        Nominale\t        log₂(N)\t            Encodage binaire de l’index\n",
    "# Hashing Encoding\t        Nominale\t        n_components\t    Hash des modalités sur colonnes fixes\n",
    "# Label Encoding\t        Ordinale\t        1\t                Entier arbitraire\n",
    "# Ordinal Encoding\t        Ordinale\t        1\t                Rang croissant des modalités\n",
    "# Target Encoding\t        Nominale/Ordinale\t1\t                Moyenne de la cible par modalité\n",
    "# Mean Encoding\t            Nominale/Ordinale\t1\t                Idem Target Encoding\n",
    "# Frequency Encoding\t    Nominale/Ordinale\t1\t                Fréquence d'apparition\n",
    "# Leave-One-Out\t            Nominale/Ordinale\t1\t                Moyenne de la cible, sauf ligne courante\n",
    "# James-Stein Encoding\t    Nominale/Ordinale\t1\t                Moyenne pondérée par variance intercatégorie\n",
    "# M-Estimate Encoding\t    Nominale/Ordinale\t1\t                Moyenne cible lissée vers moyenne globale\n",
    "# Probability Ratio\t        Ordinale, binaire\t1\t                Log du ratio de probas classe 1 / classe 0\n",
    "# WOE Encoding\t            Ordinale, binaire\t1\t                Log( %positif / %négatif )\n",
    "# Thermometer Encoding\t    Ordinale\t        N\t                1 si la modalité est ≤ à une valeur\n",
    "# Trigonométrique (sin/cos)\tCyclique\t        2\t                Encode la cyclicité\n",
    "# Fourier / Radial\t        Cyclique\t        Variable\t        Approximation périodique (base)\n",
    "ohe_enc_col = ['']\n",
    "ohe_enc = OneHotEncoder(handle_unknown='ignore', sparse_output = False)\n",
    "# Appliquer OneHotEncoder\n",
    "X_train_enc_cat = ohe_enc.fit_transform(X_train[ohe_enc_col])\n",
    "X_test_enc_cat = ohe_enc.transform(X_test[ohe_enc_col])\n",
    "# Ajout des colonnes encodées à un DataFrame car le resultat de enc.fit/transform estun ndarray sans index/colonnes\n",
    "X_train_cat_df = pd.DataFrame(X_train_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_train.index)\n",
    "X_test_cat_df = pd.DataFrame(X_test_enc_cat, columns=ohe_enc.get_feature_names_out(ohe_enc_col), index=X_test.index)\n",
    "# Suppression des colonnes catégoriques originales et ajout des colonnes encodées\n",
    "X_train = pd.concat([X_train.drop(columns=ohe_enc_col), X_train_cat_df], axis=1)\n",
    "X_test = pd.concat([X_test.drop(columns=ohe_enc_col), X_test_cat_df], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
